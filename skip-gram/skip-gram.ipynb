{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.utils.data as tud  \n",
    "from torch.nn.parameter import Parameter  #参数更新和优化函数\n",
    "from collections import Counter \n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import scipy  #\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数\n",
    "\n",
    "# 负例采样就是Skip-Gram模型的输出不是周围词的概率了，是正例和负例的概率\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    " \n",
    "K = 3   # 负样本随机采样数量\n",
    "C = 1    # 周围单词的数量\n",
    "NUM_EPOCHS = 20\n",
    "VOCAB_SIZE = 56000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2 \n",
    "EMBEDDING_SIZE = 100\n",
    "#对应的维度\n",
    "\n",
    "LOG_FILE = \"word-embedding.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  re\n",
    "file = open(\"train.txt\")\n",
    "line = file.readline()\n",
    "punctuation = '！，；：？“”\\'、；《》（）。—'\n",
    "results = []\n",
    "text = []\n",
    "word_to_sentence = {}\n",
    "word_to_sentence = defaultdict(lambda: set([]))\n",
    "#!!!上面这种操作可以将word_to_dicts的相应内容全部定义为[]\n",
    "\n",
    "while  line:\n",
    "    line = re.sub('['+punctuation+']','',line)\n",
    "    currents = line.split()\n",
    "    #print(currents)\n",
    "    text.extend(currents)\n",
    "    results.append(currents)\n",
    "    for  i  in  range(len(currents)):\n",
    "        word_to_sentence[currents[i]].add(len(results)-1)\n",
    "    line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55107"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55108\n"
     ]
    }
   ],
   "source": [
    "#词语对应的编号\n",
    "vocab = dict(Counter(text).most_common(VOCAB_SIZE-1))\n",
    "#词语出现的次数\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "#目前这里面的[\"<unk>\"]对应的内容是必为0的内容\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "#idx_to_word为所有切出来的单词构成的list\n",
    "#word_to_idx为将单词进行相应的编号\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "#word_counts为所有对应的单词构成的相应的矩阵\n",
    "#这里word_counts共有对应的483个单词\n",
    "print(len(word_counts))\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "#计算词频，按照原文转换为3/4次方\n",
    "word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "#词频为词频/词频的总和\n",
    "#将所有的单词个数转化为一个483长度的词频矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset __init__\n"
     ]
    }
   ],
   "source": [
    "# 实现Dataloader\n",
    "class Dataset(tud.Dataset): # 继承tud.Dataset父类\n",
    "    \n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):    \n",
    "        super(Dataset, self).__init__() \n",
    "        print('Dataset __init__')\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        #依次对text当中的单词进行相应的查找\n",
    "        # get()返回指定键的值，没有则返回默认值\n",
    "        # 这里面的key -- 字典中要查找的键，default -- 如果指定键的值不存在时，返回该默认值。\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        #变成tensor类型，这里变成longtensor，也可以torch.LongTensor\n",
    "        self.word_to_idx = word_to_idx \n",
    "        self.idx_to_word = idx_to_word  \n",
    "        self.word_freqs = torch.Tensor(word_freqs) \n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.text_encoded) #所有单词的总数\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], replacement=True)\n",
    "        return center_word, pos_words, neg_words \n",
    "\n",
    "\n",
    "dataset = Dataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  \n",
    "#BATCH_SIZE = 128,每一个批次随机取出128个对应的数值，这里打包成DataLoader的主要原因是\n",
    "#便于每次取出一个相应的批次操作\n",
    "\n",
    "#!!!dataloader之中对应的数据本身没有重复，但是text之中的内容有重复，所以dataloader\n",
    "#之中的单词编号也会相应的出现重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义PyTorch模型\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "    #放入的vocab_size=30000,embed_size=100\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        print('EmbeddingModel __init__')\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "              \n",
    "        # 模型输入，输出是两个一样的矩阵参数nn.Embedding(30000, 100)\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         # 权重初始化的一种方法\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围出现过的单词 [batch_size * (c * 2)],左边找出c个词组，右边找出c个词组\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (c * 2 * K)]\n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        #print('EmbeddingModel forward')\n",
    "        batch_size = input_labels.size(0) \n",
    "       \n",
    "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2C) * embed_size \n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C*K) * embed_size\n",
    "\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)) # B * (2*C)\n",
    "        log_pos = log_pos.squeeze()\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1)\n",
    "        loss = log_pos + log_neg  # 正样本损失和负样本损失和尽量最大\n",
    "        #如果为负数的时候就是损失和尽量最小\n",
    "        #对应的大小为[batch_size]\n",
    "        #因为需要提取出来的这128个维度的单词集体操作\n",
    "        return -loss \n",
    "        #注意这里return的是-loss，最终的optimizer.step中还带有一个减号\n",
    "        #所以这里如果是当前选中的这个单词的周边单词的话的128个维度单词的梯度被减去，而周边单词的梯度被加上，\n",
    "        #而如果是这128个单词负采样的话这128个维度的单词\n",
    "    \n",
    "    # 模型训练有两个矩阵，self.in_embed和self.out_embed两个, 作者认为输入矩阵比较好，舍弃了输出矩阵\n",
    "    # 取出输入矩阵参数，self.in_embed的矩阵为正采样的相应的矩阵，self.out_embed为负采样的相应矩阵\n",
    "    def input_embeddings(self):   \n",
    "        return self.in_embed.weight.data.cpu().numpy() \n",
    "    def output_embeddings(self):\n",
    "        return self.out_embed.weight.data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingModel __init__\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "#VOCAB_SIZE = 30000,EMBEDDING_SIZE = 100\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#model = model.cuda(),这里面的LEARING_RATE=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 5.545188903808594\n",
      "epoch: 0, iter: 100, loss: 5.545193672180176\n",
      "epoch: 0, iter: 200, loss: 5.545172691345215\n",
      "epoch: 0, iter: 300, loss: 5.545185565948486\n",
      "epoch: 0, iter: 400, loss: 5.545162200927734\n",
      "epoch: 0, iter: 500, loss: 5.545172214508057\n",
      "epoch: 0, iter: 600, loss: 5.5451765060424805\n",
      "epoch: 0, iter: 700, loss: 5.545167922973633\n",
      "epoch: 0, iter: 800, loss: 5.5451741218566895\n",
      "epoch: 0, iter: 900, loss: 5.545161724090576\n",
      "epoch: 0, iter: 1000, loss: 5.545164585113525\n",
      "epoch: 0, iter: 1100, loss: 5.545170783996582\n",
      "epoch: 0, iter: 1200, loss: 5.545156955718994\n",
      "epoch: 0, iter: 1300, loss: 5.545164108276367\n",
      "epoch: 0, iter: 1400, loss: 5.545173645019531\n",
      "epoch: 0, iter: 1500, loss: 5.545166969299316\n",
      "epoch: 0, iter: 1600, loss: 5.545170783996582\n",
      "epoch: 0, iter: 1700, loss: 5.545166015625\n",
      "epoch: 0, iter: 1800, loss: 5.545152187347412\n",
      "epoch: 0, iter: 1900, loss: 5.545126914978027\n",
      "epoch: 0, iter: 2000, loss: 5.545165061950684\n",
      "epoch: 0, iter: 2100, loss: 5.545143127441406\n",
      "epoch: 0, iter: 2200, loss: 5.5451507568359375\n",
      "epoch: 0, iter: 2300, loss: 5.545144081115723\n",
      "epoch: 0, iter: 2400, loss: 5.545118808746338\n",
      "epoch: 0, iter: 2500, loss: 5.545095443725586\n",
      "epoch: 0, iter: 2600, loss: 5.545154094696045\n",
      "epoch: 0, iter: 2700, loss: 5.545180797576904\n",
      "epoch: 0, iter: 2800, loss: 5.5451788902282715\n",
      "epoch: 0, iter: 2900, loss: 5.54515266418457\n",
      "epoch: 0, iter: 3000, loss: 5.545128345489502\n",
      "epoch: 0, iter: 3100, loss: 5.545040130615234\n",
      "epoch: 0, iter: 3200, loss: 5.544981002807617\n",
      "epoch: 0, iter: 3300, loss: 5.545044898986816\n",
      "epoch: 0, iter: 3400, loss: 5.545121192932129\n",
      "epoch: 0, iter: 3500, loss: 5.54498815536499\n",
      "epoch: 0, iter: 3600, loss: 5.544925212860107\n",
      "epoch: 0, iter: 3700, loss: 5.544493198394775\n",
      "epoch: 0, iter: 3800, loss: 5.544361114501953\n",
      "epoch: 0, iter: 3900, loss: 5.545073509216309\n",
      "epoch: 0, iter: 4000, loss: 5.545095443725586\n",
      "epoch: 0, iter: 4100, loss: 5.545157432556152\n",
      "epoch: 0, iter: 4200, loss: 5.544991970062256\n",
      "epoch: 0, iter: 4300, loss: 5.544698715209961\n",
      "epoch: 0, iter: 4400, loss: 5.544905662536621\n",
      "epoch: 0, iter: 4500, loss: 5.544310569763184\n",
      "epoch: 0, iter: 4600, loss: 5.544125556945801\n",
      "epoch: 0, iter: 4700, loss: 5.5449113845825195\n",
      "epoch: 0, iter: 4800, loss: 5.5442728996276855\n",
      "epoch: 0, iter: 4900, loss: 5.545003890991211\n",
      "epoch: 0, iter: 5000, loss: 5.5449628829956055\n",
      "epoch: 0, iter: 5100, loss: 5.544164180755615\n",
      "epoch: 0, iter: 5200, loss: 5.542055606842041\n",
      "epoch: 0, iter: 5300, loss: 5.544696807861328\n",
      "epoch: 0, iter: 5400, loss: 5.543572902679443\n",
      "epoch: 0, iter: 5500, loss: 5.54318904876709\n",
      "epoch: 0, iter: 5600, loss: 5.5410075187683105\n",
      "epoch: 0, iter: 5700, loss: 5.541726112365723\n",
      "epoch: 0, iter: 5800, loss: 5.543704509735107\n",
      "epoch: 0, iter: 5900, loss: 5.544347763061523\n",
      "epoch: 0, iter: 6000, loss: 5.538483142852783\n",
      "epoch: 0, iter: 6100, loss: 5.539383411407471\n",
      "epoch: 0, iter: 6200, loss: 5.543704986572266\n",
      "epoch: 0, iter: 6300, loss: 5.54435396194458\n",
      "epoch: 0, iter: 6400, loss: 5.54257345199585\n",
      "epoch: 0, iter: 6500, loss: 5.543781280517578\n",
      "epoch: 0, iter: 6600, loss: 5.53446102142334\n",
      "epoch: 0, iter: 6700, loss: 5.53867244720459\n",
      "epoch: 0, iter: 6800, loss: 5.535845756530762\n",
      "epoch: 0, iter: 6900, loss: 5.536705017089844\n",
      "epoch: 0, iter: 7000, loss: 5.540956020355225\n",
      "epoch: 0, iter: 7100, loss: 5.541303634643555\n",
      "epoch: 0, iter: 7200, loss: 5.542795181274414\n",
      "epoch: 0, iter: 7300, loss: 5.532439231872559\n",
      "epoch = 0\n",
      "totalloss = \n",
      "tensor(40923.5703, grad_fn=<AddBackward0>)\n",
      "epoch: 1, iter: 0, loss: 5.540743827819824\n",
      "epoch: 1, iter: 100, loss: 5.536681652069092\n",
      "epoch: 1, iter: 200, loss: 5.534763813018799\n",
      "epoch: 1, iter: 300, loss: 5.540258884429932\n",
      "epoch: 1, iter: 400, loss: 5.519021034240723\n",
      "epoch: 1, iter: 500, loss: 5.532864570617676\n",
      "epoch: 1, iter: 600, loss: 5.523807048797607\n",
      "epoch: 1, iter: 700, loss: 5.5325727462768555\n",
      "epoch: 1, iter: 800, loss: 5.534339427947998\n",
      "epoch: 1, iter: 900, loss: 5.527338981628418\n",
      "epoch: 1, iter: 1000, loss: 5.532150745391846\n",
      "epoch: 1, iter: 1100, loss: 5.53327751159668\n",
      "epoch: 1, iter: 1200, loss: 5.53549337387085\n",
      "epoch: 1, iter: 1300, loss: 5.526293754577637\n",
      "epoch: 1, iter: 1400, loss: 5.527647018432617\n",
      "epoch: 1, iter: 1500, loss: 5.5201897621154785\n",
      "epoch: 1, iter: 1600, loss: 5.525404930114746\n",
      "epoch: 1, iter: 1700, loss: 5.531750679016113\n",
      "epoch: 1, iter: 1800, loss: 5.526705265045166\n",
      "epoch: 1, iter: 1900, loss: 5.511028289794922\n",
      "epoch: 1, iter: 2000, loss: 5.508773326873779\n",
      "epoch: 1, iter: 2100, loss: 5.521395683288574\n",
      "epoch: 1, iter: 2200, loss: 5.521208763122559\n",
      "epoch: 1, iter: 2300, loss: 5.526256561279297\n",
      "epoch: 1, iter: 2400, loss: 5.533781051635742\n",
      "epoch: 1, iter: 2500, loss: 5.529203414916992\n",
      "epoch: 1, iter: 2600, loss: 5.515737533569336\n",
      "epoch: 1, iter: 2700, loss: 5.510364532470703\n",
      "epoch: 1, iter: 2800, loss: 5.520748615264893\n",
      "epoch: 1, iter: 2900, loss: 5.526350498199463\n",
      "epoch: 1, iter: 3000, loss: 5.483766078948975\n",
      "epoch: 1, iter: 3100, loss: 5.498120307922363\n",
      "epoch: 1, iter: 3200, loss: 5.533675670623779\n",
      "epoch: 1, iter: 3300, loss: 5.511844635009766\n",
      "epoch: 1, iter: 3400, loss: 5.4971771240234375\n",
      "epoch: 1, iter: 3500, loss: 5.523810386657715\n",
      "epoch: 1, iter: 3600, loss: 5.504679203033447\n",
      "epoch: 1, iter: 3700, loss: 5.491725921630859\n",
      "epoch: 1, iter: 3800, loss: 5.5160231590271\n",
      "epoch: 1, iter: 3900, loss: 5.502647399902344\n",
      "epoch: 1, iter: 4000, loss: 5.497219562530518\n",
      "epoch: 1, iter: 4100, loss: 5.495754241943359\n",
      "epoch: 1, iter: 4200, loss: 5.497201919555664\n",
      "epoch: 1, iter: 4300, loss: 5.52082633972168\n",
      "epoch: 1, iter: 4400, loss: 5.530610084533691\n",
      "epoch: 1, iter: 4500, loss: 5.473202705383301\n",
      "epoch: 1, iter: 4600, loss: 5.502898216247559\n",
      "epoch: 1, iter: 4700, loss: 5.475873947143555\n",
      "epoch: 1, iter: 4800, loss: 5.481540679931641\n",
      "epoch: 1, iter: 4900, loss: 5.517435073852539\n",
      "epoch: 1, iter: 5000, loss: 5.479363441467285\n",
      "epoch: 1, iter: 5100, loss: 5.529552936553955\n",
      "epoch: 1, iter: 5200, loss: 5.517940044403076\n",
      "epoch: 1, iter: 5300, loss: 5.492599010467529\n",
      "epoch: 1, iter: 5400, loss: 5.500873565673828\n",
      "epoch: 1, iter: 5500, loss: 5.4879350662231445\n",
      "epoch: 1, iter: 5600, loss: 5.473505020141602\n",
      "epoch: 1, iter: 5700, loss: 5.491077423095703\n",
      "epoch: 1, iter: 5800, loss: 5.464412689208984\n",
      "epoch: 1, iter: 5900, loss: 5.4968695640563965\n",
      "epoch: 1, iter: 6000, loss: 5.474786281585693\n",
      "epoch: 1, iter: 6100, loss: 5.497566223144531\n",
      "epoch: 1, iter: 6200, loss: 5.494627952575684\n",
      "epoch: 1, iter: 6300, loss: 5.484755039215088\n",
      "epoch: 1, iter: 6400, loss: 5.480353355407715\n",
      "epoch: 1, iter: 6500, loss: 5.5166335105896\n",
      "epoch: 1, iter: 6600, loss: 5.482785224914551\n",
      "epoch: 1, iter: 6700, loss: 5.48873233795166\n",
      "epoch: 1, iter: 6800, loss: 5.468803405761719\n",
      "epoch: 1, iter: 6900, loss: 5.48307991027832\n",
      "epoch: 1, iter: 7000, loss: 5.471855163574219\n",
      "epoch: 1, iter: 7100, loss: 5.438383102416992\n",
      "epoch: 1, iter: 7200, loss: 5.455179214477539\n",
      "epoch: 1, iter: 7300, loss: 5.486268043518066\n",
      "epoch = 1\n",
      "totalloss = \n",
      "tensor(40656.6914, grad_fn=<AddBackward0>)\n",
      "epoch: 2, iter: 0, loss: 5.5004353523254395\n",
      "epoch: 2, iter: 100, loss: 5.507875442504883\n",
      "epoch: 2, iter: 200, loss: 5.428658962249756\n",
      "epoch: 2, iter: 300, loss: 5.463755130767822\n",
      "epoch: 2, iter: 400, loss: 5.491515159606934\n",
      "epoch: 2, iter: 500, loss: 5.484682083129883\n",
      "epoch: 2, iter: 600, loss: 5.438526153564453\n",
      "epoch: 2, iter: 700, loss: 5.471660614013672\n",
      "epoch: 2, iter: 800, loss: 5.475581169128418\n",
      "epoch: 2, iter: 900, loss: 5.458425521850586\n",
      "epoch: 2, iter: 1000, loss: 5.44765043258667\n",
      "epoch: 2, iter: 1100, loss: 5.469490051269531\n",
      "epoch: 2, iter: 1200, loss: 5.486870288848877\n",
      "epoch: 2, iter: 1300, loss: 5.450226783752441\n",
      "epoch: 2, iter: 1400, loss: 5.454575061798096\n",
      "epoch: 2, iter: 1500, loss: 5.485988140106201\n",
      "epoch: 2, iter: 2800, loss: 5.435626029968262\n",
      "epoch: 2, iter: 2900, loss: 5.4886474609375\n",
      "epoch: 2, iter: 3000, loss: 5.455200672149658\n",
      "epoch: 2, iter: 3100, loss: 5.460748195648193\n",
      "epoch: 2, iter: 3200, loss: 5.463822841644287\n",
      "epoch: 2, iter: 3300, loss: 5.429955005645752\n",
      "epoch: 2, iter: 3400, loss: 5.476994037628174\n",
      "epoch: 2, iter: 3500, loss: 5.412447452545166\n",
      "epoch: 2, iter: 3600, loss: 5.439532279968262\n",
      "epoch: 2, iter: 3700, loss: 5.424489974975586\n",
      "epoch: 2, iter: 3800, loss: 5.4102253913879395\n",
      "epoch: 2, iter: 3900, loss: 5.458543300628662\n",
      "epoch: 2, iter: 4000, loss: 5.4745917320251465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, iter: 4100, loss: 5.418550491333008\n",
      "epoch: 2, iter: 4200, loss: 5.354263782501221\n",
      "epoch: 2, iter: 4300, loss: 5.487360000610352\n",
      "epoch: 2, iter: 4400, loss: 5.435061454772949\n",
      "epoch: 2, iter: 4500, loss: 5.455600261688232\n",
      "epoch: 2, iter: 4600, loss: 5.388360977172852\n",
      "epoch: 2, iter: 4700, loss: 5.450019836425781\n",
      "epoch: 2, iter: 4800, loss: 5.457646369934082\n",
      "epoch: 2, iter: 4900, loss: 5.442681312561035\n",
      "epoch: 2, iter: 5000, loss: 5.443376541137695\n",
      "epoch: 2, iter: 5100, loss: 5.48482084274292\n",
      "epoch: 2, iter: 5200, loss: 5.469601631164551\n",
      "epoch: 2, iter: 5300, loss: 5.445549964904785\n",
      "epoch: 2, iter: 5400, loss: 5.416411399841309\n",
      "epoch: 2, iter: 5500, loss: 5.446374893188477\n",
      "epoch: 2, iter: 5600, loss: 5.410077095031738\n",
      "epoch: 2, iter: 5700, loss: 5.468776226043701\n",
      "epoch: 2, iter: 5800, loss: 5.4098968505859375\n",
      "epoch: 2, iter: 5900, loss: 5.397675037384033\n",
      "epoch: 2, iter: 6000, loss: 5.441617012023926\n",
      "epoch: 2, iter: 6100, loss: 5.435159206390381\n",
      "epoch: 2, iter: 6200, loss: 5.463455677032471\n",
      "epoch: 2, iter: 6300, loss: 5.441009998321533\n",
      "epoch: 2, iter: 6400, loss: 5.500321388244629\n",
      "epoch: 2, iter: 6500, loss: 5.490201473236084\n",
      "epoch: 2, iter: 6600, loss: 5.427361488342285\n",
      "epoch: 2, iter: 6700, loss: 5.477621078491211\n",
      "epoch: 2, iter: 6800, loss: 5.398551940917969\n",
      "epoch: 2, iter: 6900, loss: 5.429896831512451\n",
      "epoch: 2, iter: 7000, loss: 5.454798221588135\n",
      "epoch: 2, iter: 7100, loss: 5.447912216186523\n",
      "epoch: 2, iter: 7200, loss: 5.3840012550354\n",
      "epoch: 2, iter: 7300, loss: 5.460443019866943\n",
      "epoch = 2\n",
      "totalloss = \n",
      "tensor(40256.7617, grad_fn=<AddBackward0>)\n",
      "epoch: 3, iter: 0, loss: 5.408124923706055\n",
      "epoch: 3, iter: 100, loss: 5.416059970855713\n",
      "epoch: 3, iter: 200, loss: 5.419037342071533\n",
      "epoch: 3, iter: 300, loss: 5.469807147979736\n",
      "epoch: 3, iter: 400, loss: 5.380461692810059\n",
      "epoch: 3, iter: 500, loss: 5.416011810302734\n",
      "epoch: 3, iter: 600, loss: 5.426602363586426\n",
      "epoch: 3, iter: 700, loss: 5.397378921508789\n",
      "epoch: 3, iter: 800, loss: 5.405785083770752\n",
      "epoch: 3, iter: 900, loss: 5.364702224731445\n",
      "epoch: 3, iter: 1000, loss: 5.393200397491455\n",
      "epoch: 3, iter: 1100, loss: 5.3778204917907715\n",
      "epoch: 3, iter: 1200, loss: 5.44414758682251\n",
      "epoch: 3, iter: 1300, loss: 5.485275745391846\n",
      "epoch: 3, iter: 1400, loss: 5.407834529876709\n",
      "epoch: 3, iter: 1500, loss: 5.4828643798828125\n",
      "epoch: 3, iter: 1600, loss: 5.377030372619629\n",
      "epoch: 3, iter: 1700, loss: 5.49415397644043\n",
      "epoch: 3, iter: 1800, loss: 5.429243564605713\n",
      "epoch: 3, iter: 1900, loss: 5.417476654052734\n",
      "epoch: 3, iter: 2000, loss: 5.427090167999268\n",
      "epoch: 3, iter: 2100, loss: 5.4181976318359375\n",
      "epoch: 3, iter: 2200, loss: 5.391960620880127\n",
      "epoch: 3, iter: 2300, loss: 5.368813514709473\n",
      "epoch: 3, iter: 2400, loss: 5.487337112426758\n",
      "epoch: 3, iter: 2500, loss: 5.404146194458008\n",
      "epoch: 3, iter: 2600, loss: 5.404458522796631\n",
      "epoch: 3, iter: 2700, loss: 5.454407215118408\n",
      "epoch: 3, iter: 2800, loss: 5.361072540283203\n",
      "epoch: 3, iter: 2900, loss: 5.425418376922607\n",
      "epoch: 3, iter: 3000, loss: 5.416930198669434\n",
      "epoch: 3, iter: 3100, loss: 5.411472320556641\n",
      "epoch: 3, iter: 3200, loss: 5.248721122741699\n",
      "epoch: 3, iter: 3300, loss: 5.399324417114258\n",
      "epoch: 3, iter: 3400, loss: 5.387103080749512\n",
      "epoch: 3, iter: 3500, loss: 5.361146450042725\n",
      "epoch: 3, iter: 3600, loss: 5.363461017608643\n",
      "epoch: 3, iter: 3700, loss: 5.3721418380737305\n",
      "epoch: 3, iter: 3800, loss: 5.34810733795166\n",
      "epoch: 3, iter: 3900, loss: 5.428305625915527\n",
      "epoch: 3, iter: 4000, loss: 5.399216175079346\n",
      "epoch: 3, iter: 4100, loss: 5.442880630493164\n",
      "epoch: 3, iter: 4200, loss: 5.474632263183594\n",
      "epoch: 3, iter: 4300, loss: 5.350751876831055\n",
      "epoch: 3, iter: 4400, loss: 5.36470890045166\n",
      "epoch: 3, iter: 4500, loss: 5.45372200012207\n",
      "epoch: 3, iter: 4600, loss: 5.443819522857666\n",
      "epoch: 3, iter: 4700, loss: 5.400075912475586\n",
      "epoch: 3, iter: 4800, loss: 5.4902496337890625\n",
      "epoch: 3, iter: 4900, loss: 5.429314613342285\n",
      "epoch: 3, iter: 5000, loss: 5.39777946472168\n",
      "epoch: 3, iter: 5100, loss: 5.40515661239624\n",
      "epoch: 3, iter: 5200, loss: 5.4295735359191895\n",
      "epoch: 3, iter: 5300, loss: 5.3533124923706055\n",
      "epoch: 3, iter: 5400, loss: 5.312592506408691\n",
      "epoch: 3, iter: 5500, loss: 5.4311723709106445\n",
      "epoch: 3, iter: 5600, loss: 5.383377552032471\n",
      "epoch: 3, iter: 5700, loss: 5.3790364265441895\n",
      "epoch: 3, iter: 5800, loss: 5.455935001373291\n",
      "epoch: 3, iter: 5900, loss: 5.342277526855469\n",
      "epoch: 3, iter: 6000, loss: 5.425513744354248\n",
      "epoch: 3, iter: 6100, loss: 5.392392635345459\n",
      "epoch: 3, iter: 6200, loss: 5.413131237030029\n",
      "epoch: 3, iter: 6300, loss: 5.407627105712891\n",
      "epoch: 3, iter: 6400, loss: 5.40366792678833\n",
      "epoch: 3, iter: 6500, loss: 5.447062015533447\n",
      "epoch: 3, iter: 6600, loss: 5.353501796722412\n",
      "epoch: 3, iter: 6700, loss: 5.376763343811035\n",
      "epoch: 3, iter: 6800, loss: 5.316514015197754\n",
      "epoch: 3, iter: 6900, loss: 5.431666374206543\n",
      "epoch: 3, iter: 7000, loss: 5.436319351196289\n",
      "epoch: 3, iter: 7100, loss: 5.370434284210205\n",
      "epoch: 3, iter: 7200, loss: 5.363781929016113\n",
      "epoch: 3, iter: 7300, loss: 5.409085273742676\n",
      "epoch = 3\n",
      "totalloss = \n",
      "tensor(39904.5547, grad_fn=<AddBackward0>)\n",
      "epoch: 4, iter: 0, loss: 5.377900123596191\n",
      "epoch: 4, iter: 100, loss: 5.3186445236206055\n",
      "epoch: 4, iter: 200, loss: 5.398441791534424\n",
      "epoch: 4, iter: 300, loss: 5.418082237243652\n",
      "epoch: 4, iter: 400, loss: 5.360661506652832\n",
      "epoch: 4, iter: 500, loss: 5.315125942230225\n",
      "epoch: 4, iter: 600, loss: 5.361941814422607\n",
      "epoch: 4, iter: 700, loss: 5.336835861206055\n",
      "epoch: 4, iter: 800, loss: 5.366328239440918\n",
      "epoch: 4, iter: 900, loss: 5.387809753417969\n",
      "epoch: 4, iter: 1000, loss: 5.366063117980957\n",
      "epoch: 4, iter: 1100, loss: 5.4383931159973145\n",
      "epoch: 4, iter: 1200, loss: 5.343951225280762\n",
      "epoch: 4, iter: 1300, loss: 5.337161540985107\n",
      "epoch: 4, iter: 1400, loss: 5.352058410644531\n",
      "epoch: 4, iter: 1500, loss: 5.340534687042236\n",
      "epoch: 4, iter: 1600, loss: 5.329896450042725\n",
      "epoch: 4, iter: 1700, loss: 5.377192497253418\n",
      "epoch: 4, iter: 1800, loss: 5.333065986633301\n",
      "epoch: 4, iter: 1900, loss: 5.345434188842773\n",
      "epoch: 4, iter: 2000, loss: 5.309466361999512\n",
      "epoch: 4, iter: 2100, loss: 5.414735317230225\n",
      "epoch: 4, iter: 2200, loss: 5.269366264343262\n",
      "epoch: 4, iter: 2300, loss: 5.409948825836182\n",
      "epoch: 4, iter: 2400, loss: 5.37611198425293\n",
      "epoch: 4, iter: 2500, loss: 5.350517749786377\n",
      "epoch: 4, iter: 2600, loss: 5.3180413246154785\n",
      "epoch: 4, iter: 2700, loss: 5.307275772094727\n",
      "epoch: 4, iter: 2800, loss: 5.351149082183838\n",
      "epoch: 4, iter: 2900, loss: 5.3758392333984375\n",
      "epoch: 4, iter: 3000, loss: 5.2733306884765625\n",
      "epoch: 4, iter: 3100, loss: 5.374982833862305\n",
      "epoch: 4, iter: 3200, loss: 5.369543552398682\n",
      "epoch: 4, iter: 3300, loss: 5.341036796569824\n",
      "epoch: 4, iter: 3400, loss: 5.364999771118164\n",
      "epoch: 4, iter: 3500, loss: 5.302973747253418\n",
      "epoch: 4, iter: 3600, loss: 5.336846351623535\n",
      "epoch: 4, iter: 3700, loss: 5.337082862854004\n",
      "epoch: 4, iter: 3800, loss: 5.359320163726807\n",
      "epoch: 4, iter: 3900, loss: 5.377761363983154\n",
      "epoch: 4, iter: 4000, loss: 5.281587600708008\n",
      "epoch: 4, iter: 4100, loss: 5.339244842529297\n",
      "epoch: 4, iter: 4200, loss: 5.356709003448486\n",
      "epoch: 4, iter: 4300, loss: 5.3135151863098145\n",
      "epoch: 4, iter: 4400, loss: 5.322092533111572\n",
      "epoch: 4, iter: 4500, loss: 5.35452127456665\n",
      "epoch: 4, iter: 4600, loss: 5.3465189933776855\n",
      "epoch: 4, iter: 4700, loss: 5.344386100769043\n",
      "epoch: 4, iter: 4800, loss: 5.307878494262695\n",
      "epoch: 4, iter: 4900, loss: 5.25286865234375\n",
      "epoch: 4, iter: 5000, loss: 5.33080530166626\n",
      "epoch: 4, iter: 5100, loss: 5.336052894592285\n",
      "epoch: 4, iter: 5200, loss: 5.310883522033691\n",
      "epoch: 4, iter: 5300, loss: 5.353355407714844\n",
      "epoch: 4, iter: 5400, loss: 5.354142189025879\n",
      "epoch: 4, iter: 5500, loss: 5.224470615386963\n",
      "epoch: 4, iter: 5600, loss: 5.239365577697754\n",
      "epoch: 4, iter: 5700, loss: 5.35145378112793\n",
      "epoch: 4, iter: 5800, loss: 5.264530181884766\n",
      "epoch: 4, iter: 5900, loss: 5.3751540184021\n",
      "epoch: 4, iter: 6000, loss: 5.300650596618652\n",
      "epoch: 4, iter: 6100, loss: 5.2661967277526855\n",
      "epoch: 4, iter: 6200, loss: 5.247379302978516\n",
      "epoch: 4, iter: 6300, loss: 5.3726654052734375\n",
      "epoch: 4, iter: 6400, loss: 5.268462657928467\n",
      "epoch: 4, iter: 6500, loss: 5.276025772094727\n",
      "epoch: 4, iter: 6600, loss: 5.314948081970215\n",
      "epoch: 4, iter: 6700, loss: 5.3200578689575195\n",
      "epoch: 4, iter: 6800, loss: 5.321399688720703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, iter: 6900, loss: 5.2797698974609375\n",
      "epoch: 4, iter: 7000, loss: 5.320988655090332\n",
      "epoch: 4, iter: 7100, loss: 5.282883644104004\n",
      "epoch: 4, iter: 7200, loss: 5.2900519371032715\n",
      "epoch: 4, iter: 7300, loss: 5.3194756507873535\n",
      "epoch = 4\n",
      "totalloss = \n",
      "tensor(39376.1562, grad_fn=<AddBackward0>)\n",
      "epoch: 5, iter: 0, loss: 5.2775187492370605\n",
      "epoch: 5, iter: 100, loss: 5.260690689086914\n",
      "epoch: 5, iter: 200, loss: 5.32249641418457\n",
      "epoch: 5, iter: 300, loss: 5.35313081741333\n",
      "epoch: 5, iter: 400, loss: 5.364988803863525\n",
      "epoch: 5, iter: 500, loss: 5.221414566040039\n",
      "epoch: 5, iter: 600, loss: 5.2652435302734375\n",
      "epoch: 5, iter: 700, loss: 5.296438217163086\n",
      "epoch: 5, iter: 800, loss: 5.176678657531738\n",
      "epoch: 5, iter: 900, loss: 5.214909076690674\n",
      "epoch: 5, iter: 1000, loss: 5.274603843688965\n",
      "epoch: 5, iter: 1100, loss: 5.29140043258667\n",
      "epoch: 5, iter: 1200, loss: 5.247220039367676\n",
      "epoch: 5, iter: 1300, loss: 5.335747241973877\n",
      "epoch: 5, iter: 1400, loss: 5.2345871925354\n",
      "epoch: 5, iter: 1500, loss: 5.2552409172058105\n",
      "epoch: 5, iter: 1600, loss: 5.282346725463867\n",
      "epoch: 5, iter: 1700, loss: 5.268388271331787\n",
      "epoch: 5, iter: 1800, loss: 5.25064754486084\n",
      "epoch: 5, iter: 1900, loss: 5.270322322845459\n",
      "epoch: 5, iter: 2000, loss: 5.235800743103027\n",
      "epoch: 5, iter: 2100, loss: 5.229462146759033\n",
      "epoch: 5, iter: 2200, loss: 5.325692176818848\n",
      "epoch: 5, iter: 2300, loss: 5.299645900726318\n",
      "epoch: 5, iter: 2400, loss: 5.205665111541748\n",
      "epoch: 5, iter: 2500, loss: 5.277753829956055\n",
      "epoch: 5, iter: 2600, loss: 5.276658535003662\n",
      "epoch: 5, iter: 2700, loss: 5.378525733947754\n",
      "epoch: 5, iter: 2800, loss: 5.209869861602783\n",
      "epoch: 5, iter: 2900, loss: 5.220003128051758\n",
      "epoch: 5, iter: 3000, loss: 5.356879711151123\n",
      "epoch: 5, iter: 3100, loss: 5.227089881896973\n",
      "epoch: 5, iter: 3200, loss: 5.240705490112305\n",
      "epoch: 5, iter: 3300, loss: 5.267446517944336\n",
      "epoch: 5, iter: 3400, loss: 5.304938316345215\n",
      "epoch: 5, iter: 3500, loss: 5.24951171875\n",
      "epoch: 5, iter: 3600, loss: 5.238376617431641\n",
      "epoch: 5, iter: 3700, loss: 5.274020195007324\n",
      "epoch: 5, iter: 3800, loss: 5.237336158752441\n",
      "epoch: 5, iter: 3900, loss: 5.19277811050415\n",
      "epoch: 5, iter: 4000, loss: 5.247060775756836\n",
      "epoch: 5, iter: 4100, loss: 5.237401485443115\n",
      "epoch: 5, iter: 4200, loss: 5.20560359954834\n",
      "epoch: 5, iter: 4300, loss: 5.292703628540039\n",
      "epoch: 5, iter: 4400, loss: 5.183927059173584\n",
      "epoch: 5, iter: 4500, loss: 5.245643615722656\n",
      "epoch: 5, iter: 4600, loss: 5.1740875244140625\n",
      "epoch: 5, iter: 4700, loss: 5.175190448760986\n",
      "epoch: 5, iter: 4800, loss: 5.2123942375183105\n",
      "epoch: 5, iter: 4900, loss: 5.263906478881836\n",
      "epoch: 5, iter: 5000, loss: 5.20841646194458\n",
      "epoch: 5, iter: 5100, loss: 5.342611312866211\n",
      "epoch: 5, iter: 5200, loss: 5.238337993621826\n",
      "epoch: 5, iter: 5300, loss: 5.229615211486816\n",
      "epoch: 5, iter: 5400, loss: 5.207709312438965\n",
      "epoch: 5, iter: 5500, loss: 5.164948463439941\n",
      "epoch: 5, iter: 5600, loss: 5.310739517211914\n",
      "epoch: 5, iter: 5700, loss: 5.197790622711182\n",
      "epoch: 5, iter: 5800, loss: 5.265192031860352\n",
      "epoch: 5, iter: 5900, loss: 5.190005302429199\n",
      "epoch: 5, iter: 6000, loss: 5.11952018737793\n",
      "epoch: 5, iter: 6100, loss: 5.15753173828125\n",
      "epoch: 5, iter: 6200, loss: 5.267947196960449\n",
      "epoch: 5, iter: 6300, loss: 5.30507230758667\n",
      "epoch: 5, iter: 6400, loss: 5.223334312438965\n",
      "epoch: 5, iter: 6500, loss: 5.224470138549805\n",
      "epoch: 5, iter: 6600, loss: 5.214553356170654\n",
      "epoch: 5, iter: 6700, loss: 5.19456672668457\n",
      "epoch: 5, iter: 6800, loss: 5.1556196212768555\n",
      "epoch: 5, iter: 6900, loss: 5.192695140838623\n",
      "epoch: 5, iter: 7000, loss: 5.174276351928711\n",
      "epoch: 5, iter: 7100, loss: 5.215686321258545\n",
      "epoch: 5, iter: 7200, loss: 5.289801597595215\n",
      "epoch: 5, iter: 7300, loss: 5.253687381744385\n",
      "epoch = 5\n",
      "totalloss = \n",
      "tensor(38669.2070, grad_fn=<AddBackward0>)\n",
      "epoch: 6, iter: 0, loss: 5.1670427322387695\n",
      "epoch: 6, iter: 100, loss: 5.207813739776611\n",
      "epoch: 6, iter: 200, loss: 5.197505474090576\n",
      "epoch: 6, iter: 300, loss: 5.247529983520508\n",
      "epoch: 6, iter: 400, loss: 5.192093372344971\n",
      "epoch: 6, iter: 500, loss: 5.280310153961182\n",
      "epoch: 6, iter: 600, loss: 5.199431419372559\n",
      "epoch: 6, iter: 700, loss: 5.1492414474487305\n",
      "epoch: 6, iter: 800, loss: 5.195488452911377\n",
      "epoch: 6, iter: 900, loss: 5.20906925201416\n",
      "epoch: 6, iter: 1000, loss: 5.205699920654297\n",
      "epoch: 6, iter: 1100, loss: 5.211358547210693\n",
      "epoch: 6, iter: 1200, loss: 5.185800552368164\n",
      "epoch: 6, iter: 1300, loss: 5.183043003082275\n",
      "epoch: 6, iter: 1400, loss: 5.155215263366699\n",
      "epoch: 6, iter: 1500, loss: 5.104215145111084\n",
      "epoch: 6, iter: 1600, loss: 5.141859531402588\n",
      "epoch: 6, iter: 1700, loss: 5.140228748321533\n",
      "epoch: 6, iter: 1800, loss: 5.118727684020996\n",
      "epoch: 6, iter: 1900, loss: 5.2261576652526855\n",
      "epoch: 6, iter: 2000, loss: 5.233014106750488\n",
      "epoch: 6, iter: 2100, loss: 5.115332126617432\n",
      "epoch: 6, iter: 2200, loss: 5.104433536529541\n",
      "epoch: 6, iter: 2300, loss: 5.163321018218994\n",
      "epoch: 6, iter: 2400, loss: 5.0873308181762695\n",
      "epoch: 6, iter: 2500, loss: 5.201059818267822\n",
      "epoch: 6, iter: 2600, loss: 5.143232345581055\n",
      "epoch: 6, iter: 2700, loss: 5.157827854156494\n",
      "epoch: 6, iter: 2800, loss: 5.307933807373047\n",
      "epoch: 6, iter: 2900, loss: 5.171856880187988\n",
      "epoch: 6, iter: 3000, loss: 5.137456893920898\n",
      "epoch: 6, iter: 3100, loss: 5.158568382263184\n",
      "epoch: 6, iter: 3200, loss: 5.133669376373291\n",
      "epoch: 6, iter: 3300, loss: 5.109133720397949\n",
      "epoch: 6, iter: 3400, loss: 5.065967559814453\n",
      "epoch: 6, iter: 3500, loss: 5.103880882263184\n",
      "epoch: 6, iter: 3600, loss: 5.11746883392334\n",
      "epoch: 6, iter: 3700, loss: 5.137093544006348\n",
      "epoch: 6, iter: 3800, loss: 5.060568809509277\n",
      "epoch: 6, iter: 3900, loss: 5.167498588562012\n",
      "epoch: 6, iter: 4000, loss: 5.133807182312012\n",
      "epoch: 6, iter: 4100, loss: 5.1187424659729\n",
      "epoch: 6, iter: 4200, loss: 5.204174041748047\n",
      "epoch: 6, iter: 4300, loss: 5.13940954208374\n",
      "epoch: 6, iter: 4400, loss: 5.063798427581787\n",
      "epoch: 6, iter: 4500, loss: 5.2037672996521\n",
      "epoch: 6, iter: 4600, loss: 5.063671112060547\n",
      "epoch: 6, iter: 4700, loss: 5.163852691650391\n",
      "epoch: 6, iter: 4800, loss: 5.134701251983643\n",
      "epoch: 6, iter: 4900, loss: 5.111281394958496\n",
      "epoch: 6, iter: 5000, loss: 5.149333477020264\n",
      "epoch: 6, iter: 5100, loss: 5.067306041717529\n",
      "epoch: 6, iter: 5200, loss: 5.1301045417785645\n",
      "epoch: 6, iter: 5300, loss: 5.177826881408691\n",
      "epoch: 6, iter: 5400, loss: 5.127274036407471\n",
      "epoch: 6, iter: 5500, loss: 5.119960784912109\n",
      "epoch: 6, iter: 5600, loss: 5.105645179748535\n",
      "epoch: 6, iter: 5700, loss: 5.03916072845459\n",
      "epoch: 6, iter: 5800, loss: 5.115816116333008\n",
      "epoch: 6, iter: 5900, loss: 5.120221138000488\n",
      "epoch: 6, iter: 6000, loss: 5.013657569885254\n",
      "epoch: 6, iter: 6100, loss: 5.123132705688477\n",
      "epoch: 6, iter: 6200, loss: 5.216397762298584\n",
      "epoch: 6, iter: 6300, loss: 5.099376678466797\n",
      "epoch: 6, iter: 6400, loss: 5.154634475708008\n",
      "epoch: 6, iter: 6500, loss: 5.0562310218811035\n",
      "epoch: 6, iter: 6600, loss: 5.163398742675781\n",
      "epoch: 6, iter: 6700, loss: 5.162022590637207\n",
      "epoch: 6, iter: 6800, loss: 5.141539096832275\n",
      "epoch: 6, iter: 6900, loss: 5.0312347412109375\n",
      "epoch: 6, iter: 7000, loss: 5.2295660972595215\n",
      "epoch: 6, iter: 7100, loss: 5.04478645324707\n",
      "epoch: 6, iter: 7200, loss: 5.043017864227295\n",
      "epoch: 6, iter: 7300, loss: 5.0228400230407715\n",
      "epoch = 6\n",
      "totalloss = \n",
      "tensor(37882.2812, grad_fn=<AddBackward0>)\n",
      "epoch: 7, iter: 0, loss: 5.100380897521973\n",
      "epoch: 7, iter: 100, loss: 5.101301193237305\n",
      "epoch: 7, iter: 200, loss: 5.1760172843933105\n",
      "epoch: 7, iter: 300, loss: 5.083672046661377\n",
      "epoch: 7, iter: 400, loss: 5.025339126586914\n",
      "epoch: 7, iter: 500, loss: 5.045464992523193\n",
      "epoch: 7, iter: 600, loss: 5.087968349456787\n",
      "epoch: 7, iter: 700, loss: 5.112906455993652\n",
      "epoch: 7, iter: 800, loss: 5.00623083114624\n",
      "epoch: 7, iter: 900, loss: 5.078024864196777\n",
      "epoch: 7, iter: 1000, loss: 5.075288772583008\n",
      "epoch: 7, iter: 1100, loss: 5.041390895843506\n",
      "epoch: 7, iter: 1200, loss: 5.076742649078369\n",
      "epoch: 7, iter: 1300, loss: 5.095209121704102\n",
      "epoch: 7, iter: 1400, loss: 5.1007890701293945\n",
      "epoch: 7, iter: 1500, loss: 5.013995170593262\n",
      "epoch: 7, iter: 1600, loss: 5.0928802490234375\n",
      "epoch: 7, iter: 1700, loss: 5.109095573425293\n",
      "epoch: 7, iter: 1800, loss: 5.09645938873291\n",
      "epoch: 7, iter: 1900, loss: 5.155826568603516\n",
      "epoch: 7, iter: 2000, loss: 4.9886016845703125\n",
      "epoch: 7, iter: 2100, loss: 5.081479072570801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, iter: 2200, loss: 5.108485698699951\n",
      "epoch: 7, iter: 2300, loss: 5.050796031951904\n",
      "epoch: 7, iter: 2400, loss: 5.085929870605469\n",
      "epoch: 7, iter: 2500, loss: 4.964363098144531\n",
      "epoch: 7, iter: 2600, loss: 5.1462202072143555\n",
      "epoch: 7, iter: 2700, loss: 5.079452991485596\n",
      "epoch: 7, iter: 2800, loss: 5.027925491333008\n",
      "epoch: 7, iter: 2900, loss: 5.024785995483398\n",
      "epoch: 7, iter: 3000, loss: 5.0356059074401855\n",
      "epoch: 7, iter: 3100, loss: 5.017446517944336\n",
      "epoch: 7, iter: 3200, loss: 4.93335485458374\n",
      "epoch: 7, iter: 3300, loss: 5.0746750831604\n",
      "epoch: 7, iter: 3400, loss: 4.934443950653076\n",
      "epoch: 7, iter: 3500, loss: 5.086825847625732\n",
      "epoch: 7, iter: 3600, loss: 4.893255710601807\n",
      "epoch: 7, iter: 3700, loss: 4.8710432052612305\n",
      "epoch: 7, iter: 3800, loss: 5.0140886306762695\n",
      "epoch: 7, iter: 3900, loss: 4.9357709884643555\n",
      "epoch: 7, iter: 4000, loss: 5.073157787322998\n",
      "epoch: 7, iter: 4100, loss: 5.018987655639648\n",
      "epoch: 7, iter: 4200, loss: 5.010644912719727\n",
      "epoch: 7, iter: 4300, loss: 5.058835983276367\n",
      "epoch: 7, iter: 4400, loss: 5.009855270385742\n",
      "epoch: 7, iter: 4500, loss: 4.97075080871582\n",
      "epoch: 7, iter: 4600, loss: 5.029400825500488\n",
      "epoch: 7, iter: 4700, loss: 5.046605587005615\n",
      "epoch: 7, iter: 4800, loss: 4.9385085105896\n",
      "epoch: 7, iter: 4900, loss: 4.960526943206787\n",
      "epoch: 7, iter: 5000, loss: 5.082818984985352\n",
      "epoch: 7, iter: 5100, loss: 5.0111212730407715\n",
      "epoch: 7, iter: 5200, loss: 4.968393325805664\n",
      "epoch: 7, iter: 5300, loss: 5.09449577331543\n",
      "epoch: 7, iter: 5400, loss: 5.0837931632995605\n",
      "epoch: 7, iter: 5500, loss: 4.951372146606445\n",
      "epoch: 7, iter: 5600, loss: 5.052536487579346\n",
      "epoch: 7, iter: 5700, loss: 5.008145809173584\n",
      "epoch: 7, iter: 5800, loss: 5.145688533782959\n",
      "epoch: 7, iter: 5900, loss: 4.937400817871094\n",
      "epoch: 7, iter: 6000, loss: 4.936140060424805\n",
      "epoch: 7, iter: 6100, loss: 5.066008567810059\n",
      "epoch: 7, iter: 6200, loss: 5.015568256378174\n",
      "epoch: 7, iter: 6300, loss: 4.90675163269043\n",
      "epoch: 7, iter: 6400, loss: 5.061001300811768\n",
      "epoch: 7, iter: 6500, loss: 4.974643230438232\n",
      "epoch: 7, iter: 6600, loss: 5.033174514770508\n",
      "epoch: 7, iter: 6700, loss: 4.933856010437012\n",
      "epoch: 7, iter: 6800, loss: 5.028857231140137\n",
      "epoch: 7, iter: 6900, loss: 5.056092262268066\n",
      "epoch: 7, iter: 7000, loss: 4.983246803283691\n",
      "epoch: 7, iter: 7100, loss: 4.962157249450684\n",
      "epoch: 7, iter: 7200, loss: 4.895323753356934\n",
      "epoch: 7, iter: 7300, loss: 5.0031609535217285\n",
      "epoch = 7\n",
      "totalloss = \n",
      "tensor(37099.4609, grad_fn=<AddBackward0>)\n",
      "epoch: 8, iter: 0, loss: 4.972119331359863\n",
      "epoch: 8, iter: 100, loss: 5.006939888000488\n",
      "epoch: 8, iter: 200, loss: 4.867403507232666\n",
      "epoch: 8, iter: 300, loss: 4.9387969970703125\n",
      "epoch: 8, iter: 400, loss: 4.927303314208984\n",
      "epoch: 8, iter: 500, loss: 4.936368942260742\n",
      "epoch: 8, iter: 600, loss: 4.987600326538086\n",
      "epoch: 8, iter: 700, loss: 5.042679786682129\n",
      "epoch: 8, iter: 800, loss: 5.00403356552124\n",
      "epoch: 8, iter: 900, loss: 5.070021629333496\n",
      "epoch: 8, iter: 1000, loss: 5.011750221252441\n",
      "epoch: 8, iter: 1100, loss: 4.973194122314453\n",
      "epoch: 8, iter: 1200, loss: 5.002627372741699\n",
      "epoch: 8, iter: 1300, loss: 5.025527000427246\n",
      "epoch: 8, iter: 1400, loss: 4.810973167419434\n",
      "epoch: 8, iter: 1500, loss: 4.917140960693359\n",
      "epoch: 8, iter: 1600, loss: 4.953522205352783\n",
      "epoch: 8, iter: 1700, loss: 4.836701393127441\n",
      "epoch: 8, iter: 1800, loss: 5.048670291900635\n",
      "epoch: 8, iter: 1900, loss: 4.959840774536133\n",
      "epoch: 8, iter: 2000, loss: 4.915881633758545\n",
      "epoch: 8, iter: 2100, loss: 4.937768459320068\n",
      "epoch: 8, iter: 2200, loss: 4.998752117156982\n",
      "epoch: 8, iter: 2300, loss: 4.947315692901611\n",
      "epoch: 8, iter: 2400, loss: 4.872356414794922\n",
      "epoch: 8, iter: 2500, loss: 4.957408428192139\n",
      "epoch: 8, iter: 2600, loss: 4.878028869628906\n",
      "epoch: 8, iter: 2700, loss: 4.927487850189209\n",
      "epoch: 8, iter: 2800, loss: 4.979748725891113\n",
      "epoch: 8, iter: 2900, loss: 4.927609443664551\n",
      "epoch: 8, iter: 3000, loss: 4.970686912536621\n",
      "epoch: 8, iter: 3100, loss: 5.055680274963379\n",
      "epoch: 8, iter: 3200, loss: 4.877939701080322\n",
      "epoch: 8, iter: 3300, loss: 4.875150680541992\n",
      "epoch: 8, iter: 3400, loss: 5.011120796203613\n",
      "epoch: 8, iter: 3500, loss: 4.930015563964844\n",
      "epoch: 8, iter: 3600, loss: 4.970134735107422\n",
      "epoch: 8, iter: 3700, loss: 4.971895694732666\n",
      "epoch: 8, iter: 3800, loss: 4.989564418792725\n",
      "epoch: 8, iter: 3900, loss: 4.962945461273193\n",
      "epoch: 8, iter: 4000, loss: 4.894834518432617\n",
      "epoch: 8, iter: 4100, loss: 4.977580547332764\n",
      "epoch: 8, iter: 4200, loss: 4.946102142333984\n",
      "epoch: 8, iter: 4300, loss: 4.961632251739502\n",
      "epoch: 8, iter: 4400, loss: 4.957063674926758\n",
      "epoch: 8, iter: 4500, loss: 4.863292217254639\n",
      "epoch: 8, iter: 4600, loss: 4.835087776184082\n",
      "epoch: 8, iter: 4700, loss: 4.9467597007751465\n",
      "epoch: 8, iter: 4800, loss: 4.827615737915039\n",
      "epoch: 8, iter: 4900, loss: 4.885556697845459\n",
      "epoch: 8, iter: 5000, loss: 4.994329929351807\n",
      "epoch: 8, iter: 5100, loss: 4.822744369506836\n",
      "epoch: 8, iter: 5200, loss: 4.852489471435547\n",
      "epoch: 8, iter: 5300, loss: 4.974776268005371\n",
      "epoch: 8, iter: 5400, loss: 4.963762283325195\n",
      "epoch: 8, iter: 5500, loss: 4.924638748168945\n",
      "epoch: 8, iter: 5600, loss: 4.883462905883789\n",
      "epoch: 8, iter: 5700, loss: 4.843606948852539\n",
      "epoch: 8, iter: 5800, loss: 4.951240539550781\n",
      "epoch: 8, iter: 5900, loss: 4.941958427429199\n",
      "epoch: 8, iter: 6000, loss: 4.83603572845459\n",
      "epoch: 8, iter: 6100, loss: 4.962279319763184\n",
      "epoch: 8, iter: 6200, loss: 4.852046966552734\n",
      "epoch: 8, iter: 6300, loss: 4.933716773986816\n",
      "epoch: 8, iter: 6400, loss: 4.936009883880615\n",
      "epoch: 8, iter: 6500, loss: 4.961878299713135\n",
      "epoch: 8, iter: 6600, loss: 4.871379852294922\n",
      "epoch: 8, iter: 6700, loss: 4.771329879760742\n",
      "epoch: 8, iter: 6800, loss: 4.975403785705566\n",
      "epoch: 8, iter: 6900, loss: 4.865937232971191\n",
      "epoch: 8, iter: 7000, loss: 4.957390785217285\n",
      "epoch: 8, iter: 7100, loss: 4.808813095092773\n",
      "epoch: 8, iter: 7200, loss: 4.877540588378906\n",
      "epoch: 8, iter: 7300, loss: 4.830667495727539\n",
      "epoch = 8\n",
      "totalloss = \n",
      "tensor(36360.5430, grad_fn=<AddBackward0>)\n",
      "epoch: 9, iter: 0, loss: 4.91558313369751\n",
      "epoch: 9, iter: 100, loss: 4.8932976722717285\n",
      "epoch: 9, iter: 200, loss: 4.867761135101318\n",
      "epoch: 9, iter: 300, loss: 4.880315780639648\n",
      "epoch: 9, iter: 400, loss: 5.011452674865723\n",
      "epoch: 9, iter: 500, loss: 4.7784953117370605\n",
      "epoch: 9, iter: 600, loss: 4.9151811599731445\n",
      "epoch: 9, iter: 700, loss: 4.846357822418213\n",
      "epoch: 9, iter: 800, loss: 4.841575622558594\n",
      "epoch: 9, iter: 900, loss: 4.957751274108887\n",
      "epoch: 9, iter: 1000, loss: 4.894867897033691\n",
      "epoch: 9, iter: 1100, loss: 4.961162567138672\n",
      "epoch: 9, iter: 1200, loss: 4.814508438110352\n",
      "epoch: 9, iter: 1300, loss: 4.916555881500244\n",
      "epoch: 9, iter: 1400, loss: 4.680778503417969\n",
      "epoch: 9, iter: 1500, loss: 4.8688435554504395\n",
      "epoch: 9, iter: 1600, loss: 4.829404354095459\n",
      "epoch: 9, iter: 1700, loss: 4.830663204193115\n",
      "epoch: 9, iter: 1800, loss: 4.674160003662109\n",
      "epoch: 9, iter: 1900, loss: 4.961858749389648\n",
      "epoch: 9, iter: 2000, loss: 4.761757850646973\n",
      "epoch: 9, iter: 2100, loss: 4.924991607666016\n",
      "epoch: 9, iter: 2200, loss: 4.901149749755859\n",
      "epoch: 9, iter: 2300, loss: 4.800699710845947\n",
      "epoch: 9, iter: 2400, loss: 4.902416229248047\n",
      "epoch: 9, iter: 2500, loss: 4.926873207092285\n",
      "epoch: 9, iter: 2600, loss: 4.980441570281982\n",
      "epoch: 9, iter: 2700, loss: 4.756363391876221\n",
      "epoch: 9, iter: 2800, loss: 4.736471176147461\n",
      "epoch: 9, iter: 2900, loss: 4.810624599456787\n",
      "epoch: 9, iter: 3000, loss: 4.855145454406738\n",
      "epoch: 9, iter: 3100, loss: 4.814149856567383\n",
      "epoch: 9, iter: 3200, loss: 4.770008087158203\n",
      "epoch: 9, iter: 3300, loss: 4.844060897827148\n",
      "epoch: 9, iter: 3400, loss: 4.853572368621826\n",
      "epoch: 9, iter: 3500, loss: 4.795312881469727\n",
      "epoch: 9, iter: 3600, loss: 4.8578410148620605\n",
      "epoch: 9, iter: 3700, loss: 4.833920478820801\n",
      "epoch: 9, iter: 3800, loss: 4.750633716583252\n",
      "epoch: 9, iter: 3900, loss: 4.806038856506348\n",
      "epoch: 9, iter: 4000, loss: 4.939298629760742\n",
      "epoch: 9, iter: 4100, loss: 4.668185234069824\n",
      "epoch: 9, iter: 4200, loss: 4.930395126342773\n",
      "epoch: 9, iter: 4300, loss: 4.835480690002441\n",
      "epoch: 9, iter: 4400, loss: 4.951215744018555\n",
      "epoch: 9, iter: 4500, loss: 4.715071678161621\n",
      "epoch: 9, iter: 4600, loss: 4.942079067230225\n",
      "epoch: 9, iter: 4700, loss: 4.810591697692871\n",
      "epoch: 9, iter: 4800, loss: 4.8333420753479\n",
      "epoch: 9, iter: 4900, loss: 4.853616714477539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, iter: 5000, loss: 4.797247409820557\n",
      "epoch: 9, iter: 5100, loss: 4.818394660949707\n",
      "epoch: 9, iter: 5200, loss: 4.730887413024902\n",
      "epoch: 9, iter: 5300, loss: 4.905886173248291\n",
      "epoch: 9, iter: 5400, loss: 4.95833158493042\n",
      "epoch: 9, iter: 5500, loss: 4.897167682647705\n",
      "epoch: 9, iter: 5600, loss: 4.812552452087402\n",
      "epoch: 9, iter: 5700, loss: 4.822450160980225\n",
      "epoch: 9, iter: 5800, loss: 4.797471046447754\n",
      "epoch: 9, iter: 5900, loss: 4.7191362380981445\n",
      "epoch: 9, iter: 6000, loss: 4.831075668334961\n",
      "epoch: 9, iter: 6100, loss: 4.742160320281982\n",
      "epoch: 9, iter: 6200, loss: 4.8374738693237305\n",
      "epoch: 9, iter: 6300, loss: 4.737945079803467\n",
      "epoch: 9, iter: 6400, loss: 4.839054107666016\n",
      "epoch: 9, iter: 6500, loss: 4.8536553382873535\n",
      "epoch: 9, iter: 6600, loss: 4.779714584350586\n",
      "epoch: 9, iter: 6700, loss: 4.931675910949707\n",
      "epoch: 9, iter: 6800, loss: 4.813510894775391\n",
      "epoch: 9, iter: 6900, loss: 4.768136978149414\n",
      "epoch: 9, iter: 7000, loss: 4.870599746704102\n",
      "epoch: 9, iter: 7100, loss: 4.835480690002441\n",
      "epoch: 9, iter: 7200, loss: 4.911421298980713\n",
      "epoch: 9, iter: 7300, loss: 4.846747398376465\n",
      "epoch = 9\n",
      "totalloss = \n",
      "tensor(35671.7031, grad_fn=<AddBackward0>)\n",
      "epoch: 10, iter: 0, loss: 4.627218246459961\n",
      "epoch: 10, iter: 100, loss: 4.754899024963379\n",
      "epoch: 10, iter: 200, loss: 4.66655969619751\n",
      "epoch: 10, iter: 300, loss: 4.653271198272705\n",
      "epoch: 10, iter: 400, loss: 4.816547870635986\n",
      "epoch: 10, iter: 500, loss: 4.771487236022949\n",
      "epoch: 10, iter: 600, loss: 4.731342315673828\n",
      "epoch: 10, iter: 700, loss: 4.789076328277588\n",
      "epoch: 10, iter: 800, loss: 4.780961036682129\n",
      "epoch: 10, iter: 900, loss: 4.767220973968506\n",
      "epoch: 10, iter: 1000, loss: 4.691333293914795\n",
      "epoch: 10, iter: 1100, loss: 4.6387224197387695\n",
      "epoch: 10, iter: 1200, loss: 4.667627334594727\n",
      "epoch: 10, iter: 1300, loss: 4.868776798248291\n",
      "epoch: 10, iter: 1400, loss: 4.817399024963379\n",
      "epoch: 10, iter: 1500, loss: 4.717010974884033\n",
      "epoch: 10, iter: 1600, loss: 4.677484035491943\n",
      "epoch: 10, iter: 1700, loss: 4.734597682952881\n",
      "epoch: 10, iter: 1800, loss: 4.793225288391113\n",
      "epoch: 10, iter: 1900, loss: 4.811203956604004\n",
      "epoch: 10, iter: 2000, loss: 4.797253131866455\n",
      "epoch: 10, iter: 2100, loss: 4.832286834716797\n",
      "epoch: 10, iter: 2200, loss: 4.862778663635254\n",
      "epoch: 10, iter: 2300, loss: 4.790742874145508\n",
      "epoch: 10, iter: 2400, loss: 4.738848686218262\n",
      "epoch: 10, iter: 2500, loss: 4.6569929122924805\n",
      "epoch: 10, iter: 2600, loss: 4.767982482910156\n",
      "epoch: 10, iter: 2700, loss: 4.813243389129639\n",
      "epoch: 10, iter: 2800, loss: 4.830975532531738\n",
      "epoch: 10, iter: 2900, loss: 4.753542900085449\n",
      "epoch: 10, iter: 3000, loss: 4.797212600708008\n",
      "epoch: 10, iter: 3100, loss: 4.747568607330322\n",
      "epoch: 10, iter: 3200, loss: 4.660964012145996\n",
      "epoch: 10, iter: 3300, loss: 4.723087310791016\n",
      "epoch: 10, iter: 3400, loss: 4.725066661834717\n",
      "epoch: 10, iter: 3500, loss: 4.902630805969238\n",
      "epoch: 10, iter: 3600, loss: 4.648055076599121\n",
      "epoch: 10, iter: 3700, loss: 4.6991705894470215\n",
      "epoch: 10, iter: 3800, loss: 4.68458366394043\n",
      "epoch: 10, iter: 3900, loss: 4.817437171936035\n",
      "epoch: 10, iter: 4000, loss: 4.735950469970703\n",
      "epoch: 10, iter: 4100, loss: 4.7050604820251465\n",
      "epoch: 10, iter: 4200, loss: 4.655182838439941\n",
      "epoch: 10, iter: 4300, loss: 4.745506286621094\n",
      "epoch: 10, iter: 4400, loss: 4.819249153137207\n",
      "epoch: 10, iter: 4500, loss: 4.764309406280518\n",
      "epoch: 10, iter: 4600, loss: 4.731869697570801\n",
      "epoch: 10, iter: 4700, loss: 4.626321792602539\n",
      "epoch: 10, iter: 4800, loss: 4.778186321258545\n",
      "epoch: 10, iter: 4900, loss: 4.581986427307129\n",
      "epoch: 10, iter: 5000, loss: 4.737154960632324\n",
      "epoch: 10, iter: 5100, loss: 4.853913307189941\n",
      "epoch: 10, iter: 5200, loss: 4.569371700286865\n",
      "epoch: 10, iter: 5300, loss: 4.730946063995361\n",
      "epoch: 10, iter: 5400, loss: 4.779592990875244\n",
      "epoch: 10, iter: 5500, loss: 4.686028957366943\n",
      "epoch: 10, iter: 5600, loss: 4.879593372344971\n",
      "epoch: 10, iter: 5700, loss: 4.7799906730651855\n",
      "epoch: 10, iter: 5800, loss: 4.679706573486328\n",
      "epoch: 10, iter: 5900, loss: 4.68711519241333\n",
      "epoch: 10, iter: 6000, loss: 4.596343517303467\n",
      "epoch: 10, iter: 6100, loss: 4.748595714569092\n",
      "epoch: 10, iter: 6200, loss: 4.724681854248047\n",
      "epoch: 10, iter: 6300, loss: 4.757375240325928\n",
      "epoch: 10, iter: 6400, loss: 4.648340225219727\n",
      "epoch: 10, iter: 6500, loss: 4.723846435546875\n",
      "epoch: 10, iter: 6600, loss: 4.742575645446777\n",
      "epoch: 10, iter: 6700, loss: 4.7043304443359375\n",
      "epoch: 10, iter: 6800, loss: 4.770906448364258\n",
      "epoch: 10, iter: 6900, loss: 4.720738410949707\n",
      "epoch: 10, iter: 7000, loss: 4.668887138366699\n",
      "epoch: 10, iter: 7100, loss: 4.664275646209717\n",
      "epoch: 10, iter: 7200, loss: 4.754920482635498\n",
      "epoch: 10, iter: 7300, loss: 4.602264404296875\n",
      "epoch = 10\n",
      "totalloss = \n",
      "tensor(35040.4648, grad_fn=<AddBackward0>)\n",
      "epoch: 11, iter: 0, loss: 4.674671649932861\n",
      "epoch: 11, iter: 100, loss: 4.628249168395996\n",
      "epoch: 11, iter: 200, loss: 4.5527167320251465\n",
      "epoch: 11, iter: 300, loss: 4.784419059753418\n",
      "epoch: 11, iter: 400, loss: 4.717377185821533\n",
      "epoch: 11, iter: 500, loss: 4.634223461151123\n",
      "epoch: 11, iter: 600, loss: 4.7808637619018555\n",
      "epoch: 11, iter: 700, loss: 4.753916263580322\n",
      "epoch: 11, iter: 800, loss: 4.621912002563477\n",
      "epoch: 11, iter: 900, loss: 4.791485786437988\n",
      "epoch: 11, iter: 1000, loss: 4.743724346160889\n",
      "epoch: 11, iter: 1100, loss: 4.568795204162598\n",
      "epoch: 11, iter: 1200, loss: 4.738717079162598\n",
      "epoch: 11, iter: 1300, loss: 4.631926536560059\n",
      "epoch: 11, iter: 1400, loss: 4.646556854248047\n",
      "epoch: 11, iter: 1500, loss: 4.723505020141602\n",
      "epoch: 11, iter: 1600, loss: 4.810437202453613\n",
      "epoch: 11, iter: 1700, loss: 4.723763465881348\n",
      "epoch: 11, iter: 1800, loss: 4.69938325881958\n",
      "epoch: 11, iter: 1900, loss: 4.619639873504639\n",
      "epoch: 11, iter: 2000, loss: 4.577007293701172\n",
      "epoch: 11, iter: 2100, loss: 4.721601486206055\n",
      "epoch: 11, iter: 2200, loss: 4.804414749145508\n",
      "epoch: 11, iter: 2300, loss: 4.732267379760742\n",
      "epoch: 11, iter: 2400, loss: 4.569602966308594\n",
      "epoch: 11, iter: 2500, loss: 4.70623779296875\n",
      "epoch: 11, iter: 2600, loss: 4.610711574554443\n",
      "epoch: 11, iter: 2700, loss: 4.594540596008301\n",
      "epoch: 11, iter: 2800, loss: 4.687803268432617\n",
      "epoch: 11, iter: 2900, loss: 4.589516639709473\n",
      "epoch: 11, iter: 3000, loss: 4.730315685272217\n",
      "epoch: 11, iter: 3100, loss: 4.70280647277832\n",
      "epoch: 11, iter: 3200, loss: 4.606115818023682\n",
      "epoch: 11, iter: 3300, loss: 4.654763698577881\n",
      "epoch: 11, iter: 3400, loss: 4.543250560760498\n",
      "epoch: 11, iter: 3500, loss: 4.642765045166016\n",
      "epoch: 11, iter: 3600, loss: 4.71661376953125\n",
      "epoch: 11, iter: 3700, loss: 4.763431072235107\n",
      "epoch: 11, iter: 3800, loss: 4.535999774932861\n",
      "epoch: 11, iter: 3900, loss: 4.644911289215088\n",
      "epoch: 11, iter: 4000, loss: 4.707701683044434\n",
      "epoch: 11, iter: 4100, loss: 4.859681129455566\n",
      "epoch: 11, iter: 4200, loss: 4.803057670593262\n",
      "epoch: 11, iter: 4300, loss: 4.605245113372803\n",
      "epoch: 11, iter: 4400, loss: 4.637407302856445\n",
      "epoch: 11, iter: 4500, loss: 4.573820114135742\n",
      "epoch: 11, iter: 4600, loss: 4.739047527313232\n",
      "epoch: 11, iter: 4700, loss: 4.68575382232666\n",
      "epoch: 11, iter: 4800, loss: 4.667854309082031\n",
      "epoch: 11, iter: 4900, loss: 4.61857271194458\n",
      "epoch: 11, iter: 5000, loss: 4.739124774932861\n",
      "epoch: 11, iter: 5100, loss: 4.598637580871582\n",
      "epoch: 11, iter: 5200, loss: 4.662727355957031\n",
      "epoch: 11, iter: 5300, loss: 4.785898208618164\n",
      "epoch: 11, iter: 5400, loss: 4.6925787925720215\n",
      "epoch: 11, iter: 5500, loss: 4.57476282119751\n",
      "epoch: 11, iter: 5600, loss: 4.663646697998047\n",
      "epoch: 11, iter: 5700, loss: 4.708991050720215\n",
      "epoch: 11, iter: 5800, loss: 4.848272323608398\n",
      "epoch: 11, iter: 5900, loss: 4.786336421966553\n",
      "epoch: 11, iter: 6000, loss: 4.713858604431152\n",
      "epoch: 11, iter: 6100, loss: 4.677846431732178\n",
      "epoch: 11, iter: 6200, loss: 4.562707901000977\n",
      "epoch: 11, iter: 6300, loss: 4.600807189941406\n",
      "epoch: 11, iter: 6400, loss: 4.584009647369385\n",
      "epoch: 11, iter: 6500, loss: 4.71292781829834\n",
      "epoch: 11, iter: 6600, loss: 4.567606449127197\n",
      "epoch: 11, iter: 6700, loss: 4.795970916748047\n",
      "epoch: 11, iter: 6800, loss: 4.706794738769531\n",
      "epoch: 11, iter: 6900, loss: 4.3661274909973145\n",
      "epoch: 11, iter: 7000, loss: 4.62724494934082\n",
      "epoch: 11, iter: 7100, loss: 4.854332447052002\n",
      "epoch: 11, iter: 7200, loss: 4.657015800476074\n",
      "epoch: 11, iter: 7300, loss: 4.630063056945801\n",
      "epoch = 11\n",
      "totalloss = \n",
      "tensor(34453.9453, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, iter: 0, loss: 4.642845630645752\n",
      "epoch: 12, iter: 100, loss: 4.584932327270508\n",
      "epoch: 12, iter: 200, loss: 4.466526985168457\n",
      "epoch: 12, iter: 300, loss: 4.717166423797607\n",
      "epoch: 12, iter: 400, loss: 4.574451923370361\n",
      "epoch: 12, iter: 500, loss: 4.503769397735596\n",
      "epoch: 12, iter: 600, loss: 4.692620277404785\n",
      "epoch: 12, iter: 700, loss: 4.593075275421143\n",
      "epoch: 12, iter: 800, loss: 4.677763938903809\n",
      "epoch: 12, iter: 900, loss: 4.764152526855469\n",
      "epoch: 12, iter: 1000, loss: 4.4911789894104\n",
      "epoch: 12, iter: 1100, loss: 4.611827850341797\n",
      "epoch: 12, iter: 1200, loss: 4.670037746429443\n",
      "epoch: 12, iter: 1300, loss: 4.641091346740723\n",
      "epoch: 12, iter: 1400, loss: 4.526908874511719\n",
      "epoch: 12, iter: 1500, loss: 4.554900646209717\n",
      "epoch: 12, iter: 1600, loss: 4.540161609649658\n",
      "epoch: 12, iter: 1700, loss: 4.6461591720581055\n",
      "epoch: 12, iter: 1800, loss: 4.602420806884766\n",
      "epoch: 12, iter: 1900, loss: 4.43310546875\n",
      "epoch: 12, iter: 2000, loss: 4.6275553703308105\n",
      "epoch: 12, iter: 2100, loss: 4.603304386138916\n",
      "epoch: 12, iter: 2200, loss: 4.626918792724609\n",
      "epoch: 12, iter: 2300, loss: 4.56741189956665\n",
      "epoch: 12, iter: 2400, loss: 4.581202030181885\n",
      "epoch: 12, iter: 2500, loss: 4.5866618156433105\n",
      "epoch: 12, iter: 2600, loss: 4.4379448890686035\n",
      "epoch: 12, iter: 2700, loss: 4.5544753074646\n",
      "epoch: 12, iter: 2800, loss: 4.677372932434082\n",
      "epoch: 12, iter: 2900, loss: 4.495269775390625\n",
      "epoch: 12, iter: 3000, loss: 4.447368144989014\n",
      "epoch: 12, iter: 3100, loss: 4.517292499542236\n",
      "epoch: 12, iter: 3200, loss: 4.6162190437316895\n",
      "epoch: 12, iter: 3300, loss: 4.433577060699463\n",
      "epoch: 12, iter: 3400, loss: 4.497462272644043\n",
      "epoch: 12, iter: 3500, loss: 4.696198463439941\n",
      "epoch: 12, iter: 3600, loss: 4.549798488616943\n",
      "epoch: 12, iter: 3700, loss: 4.621981143951416\n",
      "epoch: 12, iter: 3800, loss: 4.534986972808838\n",
      "epoch: 12, iter: 3900, loss: 4.5607757568359375\n",
      "epoch: 12, iter: 4000, loss: 4.6840500831604\n",
      "epoch: 12, iter: 4100, loss: 4.564858436584473\n",
      "epoch: 12, iter: 4200, loss: 4.5954270362854\n",
      "epoch: 12, iter: 4300, loss: 4.655514240264893\n",
      "epoch: 12, iter: 4400, loss: 4.612796306610107\n",
      "epoch: 12, iter: 4500, loss: 4.46248722076416\n",
      "epoch: 12, iter: 4600, loss: 4.737682819366455\n",
      "epoch: 12, iter: 4700, loss: 4.639402866363525\n",
      "epoch: 12, iter: 4800, loss: 4.498139381408691\n",
      "epoch: 12, iter: 4900, loss: 4.469022274017334\n",
      "epoch: 12, iter: 5000, loss: 4.578301906585693\n",
      "epoch: 12, iter: 5100, loss: 4.680066108703613\n",
      "epoch: 12, iter: 5200, loss: 4.629797458648682\n",
      "epoch: 12, iter: 5300, loss: 4.6443257331848145\n",
      "epoch: 12, iter: 5400, loss: 4.421367645263672\n",
      "epoch: 12, iter: 5500, loss: 4.44523286819458\n",
      "epoch: 12, iter: 5600, loss: 4.601648807525635\n",
      "epoch: 12, iter: 5700, loss: 4.545222282409668\n",
      "epoch: 12, iter: 5800, loss: 4.612872123718262\n",
      "epoch: 12, iter: 5900, loss: 4.453174114227295\n",
      "epoch: 12, iter: 6000, loss: 4.688991546630859\n",
      "epoch: 12, iter: 6100, loss: 4.47519063949585\n",
      "epoch: 12, iter: 6200, loss: 4.450744152069092\n",
      "epoch: 12, iter: 6300, loss: 4.5875325202941895\n",
      "epoch: 12, iter: 6400, loss: 4.613475799560547\n",
      "epoch: 12, iter: 6500, loss: 4.632885456085205\n",
      "epoch: 12, iter: 6600, loss: 4.688675403594971\n",
      "epoch: 12, iter: 6700, loss: 4.6936492919921875\n",
      "epoch: 12, iter: 6800, loss: 4.4477105140686035\n",
      "epoch: 12, iter: 6900, loss: 4.537959575653076\n",
      "epoch: 12, iter: 7000, loss: 4.601103782653809\n",
      "epoch: 12, iter: 7100, loss: 4.599493980407715\n",
      "epoch: 12, iter: 7200, loss: 4.48638916015625\n",
      "epoch: 12, iter: 7300, loss: 4.390807151794434\n",
      "epoch = 12\n",
      "totalloss = \n",
      "tensor(33916.7266, grad_fn=<AddBackward0>)\n",
      "epoch: 13, iter: 0, loss: 4.56403923034668\n",
      "epoch: 13, iter: 100, loss: 4.541797161102295\n",
      "epoch: 13, iter: 200, loss: 4.3959269523620605\n",
      "epoch: 13, iter: 300, loss: 4.549854278564453\n",
      "epoch: 13, iter: 400, loss: 4.547591686248779\n",
      "epoch: 13, iter: 500, loss: 4.541626930236816\n",
      "epoch: 13, iter: 600, loss: 4.444670677185059\n",
      "epoch: 13, iter: 700, loss: 4.512141704559326\n",
      "epoch: 13, iter: 800, loss: 4.713052749633789\n",
      "epoch: 13, iter: 900, loss: 4.438748359680176\n",
      "epoch: 13, iter: 1000, loss: 4.642998695373535\n",
      "epoch: 13, iter: 1100, loss: 4.563772201538086\n",
      "epoch: 13, iter: 1200, loss: 4.458706855773926\n",
      "epoch: 13, iter: 1300, loss: 4.536825180053711\n",
      "epoch: 13, iter: 1400, loss: 4.56870174407959\n",
      "epoch: 13, iter: 1500, loss: 4.6269073486328125\n",
      "epoch: 13, iter: 1600, loss: 4.530483245849609\n",
      "epoch: 13, iter: 1700, loss: 4.489170551300049\n",
      "epoch: 13, iter: 1800, loss: 4.452698707580566\n",
      "epoch: 13, iter: 1900, loss: 4.658893585205078\n",
      "epoch: 13, iter: 2000, loss: 4.602696895599365\n",
      "epoch: 13, iter: 2100, loss: 4.503551006317139\n",
      "epoch: 13, iter: 2200, loss: 4.497714519500732\n",
      "epoch: 13, iter: 2300, loss: 4.450314521789551\n",
      "epoch: 13, iter: 2400, loss: 4.533111095428467\n",
      "epoch: 13, iter: 2500, loss: 4.299651145935059\n",
      "epoch: 13, iter: 2600, loss: 4.525745391845703\n",
      "epoch: 13, iter: 2700, loss: 4.6585516929626465\n",
      "epoch: 13, iter: 2800, loss: 4.534428119659424\n",
      "epoch: 13, iter: 2900, loss: 4.595958232879639\n",
      "epoch: 13, iter: 3000, loss: 4.505762100219727\n",
      "epoch: 13, iter: 3100, loss: 4.415345668792725\n",
      "epoch: 13, iter: 3200, loss: 4.547817707061768\n",
      "epoch: 13, iter: 3300, loss: 4.508500099182129\n",
      "epoch: 13, iter: 3400, loss: 4.529284477233887\n",
      "epoch: 13, iter: 3500, loss: 4.649466037750244\n",
      "epoch: 13, iter: 3600, loss: 4.591253280639648\n",
      "epoch: 13, iter: 3700, loss: 4.463078498840332\n",
      "epoch: 13, iter: 3800, loss: 4.556239604949951\n",
      "epoch: 13, iter: 3900, loss: 4.333452224731445\n",
      "epoch: 13, iter: 4000, loss: 4.635557651519775\n",
      "epoch: 13, iter: 4100, loss: 4.663142681121826\n",
      "epoch: 13, iter: 4200, loss: 4.464993476867676\n",
      "epoch: 13, iter: 4300, loss: 4.348276615142822\n",
      "epoch: 13, iter: 4400, loss: 4.518058776855469\n",
      "epoch: 13, iter: 4500, loss: 4.615974426269531\n",
      "epoch: 13, iter: 4600, loss: 4.478534698486328\n",
      "epoch: 13, iter: 4700, loss: 4.482595443725586\n",
      "epoch: 13, iter: 4800, loss: 4.450291633605957\n",
      "epoch: 13, iter: 4900, loss: 4.52144193649292\n",
      "epoch: 13, iter: 5000, loss: 4.564913272857666\n",
      "epoch: 13, iter: 5100, loss: 4.4810872077941895\n",
      "epoch: 13, iter: 5200, loss: 4.4058732986450195\n",
      "epoch: 13, iter: 5300, loss: 4.473819255828857\n",
      "epoch: 13, iter: 5400, loss: 4.4617919921875\n",
      "epoch: 13, iter: 5500, loss: 4.436826229095459\n",
      "epoch: 13, iter: 5600, loss: 4.632593154907227\n",
      "epoch: 13, iter: 5700, loss: 4.505650520324707\n",
      "epoch: 13, iter: 5800, loss: 4.406375885009766\n",
      "epoch: 13, iter: 5900, loss: 4.5090789794921875\n",
      "epoch: 13, iter: 6000, loss: 4.477654933929443\n",
      "epoch: 13, iter: 6100, loss: 4.631335258483887\n",
      "epoch: 13, iter: 6200, loss: 4.410173416137695\n",
      "epoch: 13, iter: 6300, loss: 4.501924514770508\n",
      "epoch: 13, iter: 6400, loss: 4.438384532928467\n",
      "epoch: 13, iter: 6500, loss: 4.3827924728393555\n",
      "epoch: 13, iter: 6600, loss: 4.694665908813477\n",
      "epoch: 13, iter: 6700, loss: 4.444578170776367\n",
      "epoch: 13, iter: 6800, loss: 4.458426475524902\n",
      "epoch: 13, iter: 6900, loss: 4.519025802612305\n",
      "epoch: 13, iter: 7000, loss: 4.587712287902832\n",
      "epoch: 13, iter: 7100, loss: 4.571846008300781\n",
      "epoch: 13, iter: 7200, loss: 4.451440811157227\n",
      "epoch: 13, iter: 7300, loss: 4.456438064575195\n",
      "epoch = 13\n",
      "totalloss = \n",
      "tensor(33402.8672, grad_fn=<AddBackward0>)\n",
      "epoch: 14, iter: 0, loss: 4.4514641761779785\n",
      "epoch: 14, iter: 100, loss: 4.397013187408447\n",
      "epoch: 14, iter: 200, loss: 4.3977155685424805\n",
      "epoch: 14, iter: 300, loss: 4.584126949310303\n",
      "epoch: 14, iter: 400, loss: 4.640631675720215\n",
      "epoch: 14, iter: 500, loss: 4.5013885498046875\n",
      "epoch: 14, iter: 600, loss: 4.436230659484863\n",
      "epoch: 14, iter: 700, loss: 4.590800762176514\n",
      "epoch: 14, iter: 800, loss: 4.556258678436279\n",
      "epoch: 14, iter: 900, loss: 4.3200364112854\n",
      "epoch: 14, iter: 1000, loss: 4.47218656539917\n",
      "epoch: 14, iter: 1100, loss: 4.645010948181152\n",
      "epoch: 14, iter: 1200, loss: 4.419578552246094\n",
      "epoch: 14, iter: 1300, loss: 4.4024658203125\n",
      "epoch: 14, iter: 1400, loss: 4.595185279846191\n",
      "epoch: 14, iter: 1500, loss: 4.492879867553711\n",
      "epoch: 14, iter: 1600, loss: 4.367715835571289\n",
      "epoch: 14, iter: 1700, loss: 4.508953094482422\n",
      "epoch: 14, iter: 1800, loss: 4.494597911834717\n",
      "epoch: 14, iter: 1900, loss: 4.552240371704102\n",
      "epoch: 14, iter: 2000, loss: 4.425178050994873\n",
      "epoch: 14, iter: 2100, loss: 4.45890474319458\n",
      "epoch: 14, iter: 2200, loss: 4.52109432220459\n",
      "epoch: 14, iter: 2300, loss: 4.595982551574707\n",
      "epoch: 14, iter: 2400, loss: 4.382899284362793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, iter: 2500, loss: 4.58150577545166\n",
      "epoch: 14, iter: 2600, loss: 4.303423881530762\n",
      "epoch: 14, iter: 2700, loss: 4.525180816650391\n",
      "epoch: 14, iter: 2800, loss: 4.405501365661621\n",
      "epoch: 14, iter: 2900, loss: 4.593067646026611\n",
      "epoch: 14, iter: 3000, loss: 4.478621959686279\n",
      "epoch: 14, iter: 3100, loss: 4.53743314743042\n",
      "epoch: 14, iter: 3200, loss: 4.5542426109313965\n",
      "epoch: 14, iter: 3300, loss: 4.462948322296143\n",
      "epoch: 14, iter: 3400, loss: 4.446147918701172\n",
      "epoch: 14, iter: 3500, loss: 4.410549163818359\n",
      "epoch: 14, iter: 3600, loss: 4.377466201782227\n",
      "epoch: 14, iter: 3700, loss: 4.644991874694824\n",
      "epoch: 14, iter: 3800, loss: 4.381047248840332\n",
      "epoch: 14, iter: 3900, loss: 4.348536491394043\n",
      "epoch: 14, iter: 4000, loss: 4.388583660125732\n",
      "epoch: 14, iter: 4100, loss: 4.519956588745117\n",
      "epoch: 14, iter: 4200, loss: 4.618917465209961\n",
      "epoch: 14, iter: 4300, loss: 4.49232292175293\n",
      "epoch: 14, iter: 4400, loss: 4.414427757263184\n",
      "epoch: 14, iter: 4500, loss: 4.430473804473877\n",
      "epoch: 14, iter: 4600, loss: 4.413550853729248\n",
      "epoch: 14, iter: 4700, loss: 4.478002548217773\n",
      "epoch: 14, iter: 4800, loss: 4.470826625823975\n",
      "epoch: 14, iter: 4900, loss: 4.515750885009766\n",
      "epoch: 14, iter: 5000, loss: 4.602780818939209\n",
      "epoch: 14, iter: 5100, loss: 4.423647880554199\n",
      "epoch: 14, iter: 5200, loss: 4.3567657470703125\n",
      "epoch: 14, iter: 5300, loss: 4.630934238433838\n",
      "epoch: 14, iter: 5400, loss: 4.52764892578125\n",
      "epoch: 14, iter: 5500, loss: 4.412874221801758\n",
      "epoch: 14, iter: 5600, loss: 4.361101150512695\n",
      "epoch: 14, iter: 5700, loss: 4.338597774505615\n",
      "epoch: 14, iter: 5800, loss: 4.3842010498046875\n",
      "epoch: 14, iter: 5900, loss: 4.408209800720215\n",
      "epoch: 14, iter: 6000, loss: 4.414180755615234\n",
      "epoch: 14, iter: 6100, loss: 4.283021450042725\n",
      "epoch: 14, iter: 6200, loss: 4.41823673248291\n",
      "epoch: 14, iter: 6300, loss: 4.394854545593262\n",
      "epoch: 14, iter: 6400, loss: 4.38247013092041\n",
      "epoch: 14, iter: 6500, loss: 4.573660850524902\n",
      "epoch: 14, iter: 6600, loss: 4.424070835113525\n",
      "epoch: 14, iter: 6700, loss: 4.440093040466309\n",
      "epoch: 14, iter: 6800, loss: 4.586593151092529\n",
      "epoch: 14, iter: 6900, loss: 4.380572319030762\n",
      "epoch: 14, iter: 7000, loss: 4.240815162658691\n",
      "epoch: 14, iter: 7100, loss: 4.327568054199219\n",
      "epoch: 14, iter: 7200, loss: 4.488944053649902\n",
      "epoch: 14, iter: 7300, loss: 4.402886867523193\n",
      "epoch = 14\n",
      "totalloss = \n",
      "tensor(32936.3633, grad_fn=<AddBackward0>)\n",
      "epoch: 15, iter: 0, loss: 4.613670349121094\n",
      "epoch: 15, iter: 100, loss: 4.479087829589844\n",
      "epoch: 15, iter: 200, loss: 4.354703426361084\n",
      "epoch: 15, iter: 300, loss: 4.456886291503906\n",
      "epoch: 15, iter: 400, loss: 4.4221272468566895\n",
      "epoch: 15, iter: 500, loss: 4.459290504455566\n",
      "epoch: 15, iter: 600, loss: 4.490062713623047\n",
      "epoch: 15, iter: 700, loss: 4.338173866271973\n",
      "epoch: 15, iter: 800, loss: 4.4022369384765625\n",
      "epoch: 15, iter: 900, loss: 4.437356948852539\n",
      "epoch: 15, iter: 1000, loss: 4.472867012023926\n",
      "epoch: 15, iter: 1100, loss: 4.509652137756348\n",
      "epoch: 15, iter: 1200, loss: 4.300555229187012\n",
      "epoch: 15, iter: 1300, loss: 4.4948296546936035\n",
      "epoch: 15, iter: 1400, loss: 4.412503719329834\n",
      "epoch: 15, iter: 1500, loss: 4.456298828125\n",
      "epoch: 15, iter: 1600, loss: 4.557415008544922\n",
      "epoch: 15, iter: 1700, loss: 4.446534156799316\n",
      "epoch: 15, iter: 1800, loss: 4.391155242919922\n",
      "epoch: 15, iter: 1900, loss: 4.373342514038086\n",
      "epoch: 15, iter: 2000, loss: 4.291159629821777\n",
      "epoch: 15, iter: 2100, loss: 4.388291835784912\n",
      "epoch: 15, iter: 2200, loss: 4.26215934753418\n",
      "epoch: 15, iter: 2300, loss: 4.374882221221924\n",
      "epoch: 15, iter: 2400, loss: 4.502744197845459\n",
      "epoch: 15, iter: 2500, loss: 4.612048149108887\n",
      "epoch: 15, iter: 2600, loss: 4.531224250793457\n",
      "epoch: 15, iter: 2700, loss: 4.304174423217773\n",
      "epoch: 15, iter: 2800, loss: 4.406732559204102\n",
      "epoch: 15, iter: 2900, loss: 4.242252826690674\n",
      "epoch: 15, iter: 3000, loss: 4.496301651000977\n",
      "epoch: 15, iter: 3100, loss: 4.347798824310303\n",
      "epoch: 15, iter: 3200, loss: 4.450098037719727\n",
      "epoch: 15, iter: 3300, loss: 4.30294132232666\n",
      "epoch: 15, iter: 3400, loss: 4.471858024597168\n",
      "epoch: 15, iter: 3500, loss: 4.3976569175720215\n",
      "epoch: 15, iter: 3600, loss: 4.361997604370117\n",
      "epoch: 15, iter: 3700, loss: 4.49153470993042\n",
      "epoch: 15, iter: 3800, loss: 4.2758989334106445\n",
      "epoch: 15, iter: 3900, loss: 4.38189697265625\n",
      "epoch: 15, iter: 4000, loss: 4.424605846405029\n",
      "epoch: 15, iter: 4100, loss: 4.3339948654174805\n",
      "epoch: 15, iter: 4200, loss: 4.565384387969971\n",
      "epoch: 15, iter: 4300, loss: 4.35914421081543\n",
      "epoch: 15, iter: 4400, loss: 4.528571128845215\n",
      "epoch: 15, iter: 4500, loss: 4.316352844238281\n",
      "epoch: 15, iter: 4600, loss: 4.658532619476318\n",
      "epoch: 15, iter: 4700, loss: 4.386198043823242\n",
      "epoch: 15, iter: 4800, loss: 4.456442356109619\n",
      "epoch: 15, iter: 4900, loss: 4.461895942687988\n",
      "epoch: 15, iter: 5000, loss: 4.415090560913086\n",
      "epoch: 15, iter: 5100, loss: 4.402216911315918\n",
      "epoch: 15, iter: 5200, loss: 4.486927509307861\n",
      "epoch: 15, iter: 5300, loss: 4.255325794219971\n",
      "epoch: 15, iter: 5400, loss: 4.377163887023926\n",
      "epoch: 15, iter: 5500, loss: 4.460606098175049\n",
      "epoch: 15, iter: 5600, loss: 4.4104509353637695\n",
      "epoch: 15, iter: 5700, loss: 4.283990859985352\n",
      "epoch: 15, iter: 5800, loss: 4.329575061798096\n",
      "epoch: 15, iter: 5900, loss: 4.463281631469727\n",
      "epoch: 15, iter: 6000, loss: 4.350801467895508\n",
      "epoch: 15, iter: 6100, loss: 4.378343105316162\n",
      "epoch: 15, iter: 6200, loss: 4.392451286315918\n",
      "epoch: 15, iter: 6300, loss: 4.3588762283325195\n",
      "epoch: 15, iter: 6400, loss: 4.402637481689453\n",
      "epoch: 15, iter: 6500, loss: 4.477159023284912\n",
      "epoch: 15, iter: 6600, loss: 4.498959541320801\n",
      "epoch: 15, iter: 6700, loss: 4.394913673400879\n",
      "epoch: 15, iter: 6800, loss: 4.556807518005371\n",
      "epoch: 15, iter: 6900, loss: 4.500783920288086\n",
      "epoch: 15, iter: 7000, loss: 4.455798149108887\n",
      "epoch: 15, iter: 7100, loss: 4.435786724090576\n",
      "epoch: 15, iter: 7200, loss: 4.277557849884033\n",
      "epoch: 15, iter: 7300, loss: 4.203497886657715\n",
      "epoch = 15\n",
      "totalloss = \n",
      "tensor(32493.6758, grad_fn=<AddBackward0>)\n",
      "epoch: 16, iter: 0, loss: 4.317986965179443\n",
      "epoch: 16, iter: 100, loss: 4.421555519104004\n",
      "epoch: 16, iter: 200, loss: 4.255787372589111\n",
      "epoch: 16, iter: 300, loss: 4.432809829711914\n",
      "epoch: 16, iter: 400, loss: 4.305794715881348\n",
      "epoch: 16, iter: 500, loss: 4.462967395782471\n",
      "epoch: 16, iter: 600, loss: 4.35069465637207\n",
      "epoch: 16, iter: 700, loss: 4.3635029792785645\n",
      "epoch: 16, iter: 800, loss: 4.574605941772461\n",
      "epoch: 16, iter: 900, loss: 4.503325462341309\n",
      "epoch: 16, iter: 1000, loss: 4.461667537689209\n",
      "epoch: 16, iter: 1100, loss: 4.256399631500244\n",
      "epoch: 16, iter: 1200, loss: 4.1314263343811035\n",
      "epoch: 16, iter: 1300, loss: 4.378162384033203\n",
      "epoch: 16, iter: 1400, loss: 4.378623962402344\n",
      "epoch: 16, iter: 1500, loss: 4.307529449462891\n",
      "epoch: 16, iter: 1600, loss: 4.271503448486328\n",
      "epoch: 16, iter: 1700, loss: 4.409214496612549\n",
      "epoch: 16, iter: 1800, loss: 4.525580883026123\n",
      "epoch: 16, iter: 1900, loss: 4.516092777252197\n",
      "epoch: 16, iter: 2000, loss: 4.376835346221924\n",
      "epoch: 16, iter: 2100, loss: 4.35750150680542\n",
      "epoch: 16, iter: 2200, loss: 4.506460666656494\n",
      "epoch: 16, iter: 2300, loss: 4.307794094085693\n",
      "epoch: 16, iter: 2400, loss: 4.501491546630859\n",
      "epoch: 16, iter: 2500, loss: 4.197470188140869\n",
      "epoch: 16, iter: 2600, loss: 4.378012180328369\n",
      "epoch: 16, iter: 2700, loss: 4.278409481048584\n",
      "epoch: 16, iter: 2800, loss: 4.211769104003906\n",
      "epoch: 16, iter: 2900, loss: 4.390873432159424\n",
      "epoch: 16, iter: 3000, loss: 4.307235240936279\n",
      "epoch: 16, iter: 3100, loss: 4.309652805328369\n",
      "epoch: 16, iter: 3200, loss: 4.335078716278076\n",
      "epoch: 16, iter: 3300, loss: 4.374096870422363\n",
      "epoch: 16, iter: 3400, loss: 4.351004600524902\n",
      "epoch: 16, iter: 3500, loss: 4.379576683044434\n",
      "epoch: 16, iter: 3600, loss: 4.29420804977417\n",
      "epoch: 16, iter: 3700, loss: 4.327279567718506\n",
      "epoch: 16, iter: 3800, loss: 4.358489036560059\n",
      "epoch: 16, iter: 3900, loss: 4.495539665222168\n",
      "epoch: 16, iter: 4000, loss: 4.333835124969482\n",
      "epoch: 16, iter: 4100, loss: 4.287813186645508\n",
      "epoch: 16, iter: 4200, loss: 4.1366376876831055\n",
      "epoch: 16, iter: 4300, loss: 4.415311813354492\n",
      "epoch: 16, iter: 4400, loss: 4.3172383308410645\n",
      "epoch: 16, iter: 4500, loss: 4.186065196990967\n",
      "epoch: 16, iter: 4600, loss: 4.315540313720703\n",
      "epoch: 16, iter: 4700, loss: 4.306663513183594\n",
      "epoch: 16, iter: 4800, loss: 4.333765983581543\n",
      "epoch: 16, iter: 4900, loss: 4.378196716308594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, iter: 5000, loss: 4.376509666442871\n",
      "epoch: 16, iter: 5100, loss: 4.510918617248535\n",
      "epoch: 16, iter: 5200, loss: 4.246950149536133\n",
      "epoch: 16, iter: 5300, loss: 4.318685531616211\n",
      "epoch: 16, iter: 5400, loss: 4.451239585876465\n",
      "epoch: 16, iter: 5500, loss: 4.379627227783203\n",
      "epoch: 16, iter: 5600, loss: 4.242471694946289\n",
      "epoch: 16, iter: 5700, loss: 4.368413925170898\n",
      "epoch: 16, iter: 5800, loss: 4.316682815551758\n",
      "epoch: 16, iter: 5900, loss: 4.127659797668457\n",
      "epoch: 16, iter: 6000, loss: 4.359257698059082\n",
      "epoch: 16, iter: 6100, loss: 4.483580589294434\n",
      "epoch: 16, iter: 6200, loss: 4.2714948654174805\n",
      "epoch: 16, iter: 6300, loss: 4.225829601287842\n",
      "epoch: 16, iter: 6400, loss: 4.333707809448242\n",
      "epoch: 16, iter: 6500, loss: 4.493443489074707\n",
      "epoch: 16, iter: 6600, loss: 4.471436023712158\n",
      "epoch: 16, iter: 6700, loss: 4.4066596031188965\n",
      "epoch: 16, iter: 6800, loss: 4.335264682769775\n",
      "epoch: 16, iter: 6900, loss: 4.193861484527588\n",
      "epoch: 16, iter: 7000, loss: 4.389949798583984\n",
      "epoch: 16, iter: 7100, loss: 4.33143949508667\n",
      "epoch: 16, iter: 7200, loss: 4.265584468841553\n",
      "epoch: 16, iter: 7300, loss: 4.305593490600586\n",
      "epoch = 16\n",
      "totalloss = \n",
      "tensor(32082.3535, grad_fn=<AddBackward0>)\n",
      "epoch: 17, iter: 0, loss: 4.414838790893555\n",
      "epoch: 17, iter: 100, loss: 4.362582206726074\n",
      "epoch: 17, iter: 200, loss: 4.275115966796875\n",
      "epoch: 17, iter: 300, loss: 4.287565231323242\n",
      "epoch: 17, iter: 400, loss: 4.365227699279785\n",
      "epoch: 17, iter: 500, loss: 4.302246570587158\n",
      "epoch: 17, iter: 600, loss: 4.150060176849365\n",
      "epoch: 17, iter: 700, loss: 4.374917984008789\n",
      "epoch: 17, iter: 800, loss: 4.082500457763672\n",
      "epoch: 17, iter: 900, loss: 4.2046284675598145\n",
      "epoch: 17, iter: 1000, loss: 4.315873146057129\n",
      "epoch: 17, iter: 1100, loss: 4.099881172180176\n",
      "epoch: 17, iter: 1200, loss: 4.343046188354492\n",
      "epoch: 17, iter: 1300, loss: 4.316289901733398\n",
      "epoch: 17, iter: 1400, loss: 4.250888824462891\n",
      "epoch: 17, iter: 1500, loss: 4.079750061035156\n",
      "epoch: 17, iter: 1600, loss: 4.401801109313965\n",
      "epoch: 17, iter: 1700, loss: 4.373926162719727\n",
      "epoch: 17, iter: 1800, loss: 4.241377830505371\n",
      "epoch: 17, iter: 1900, loss: 4.329653739929199\n",
      "epoch: 17, iter: 2000, loss: 4.286079406738281\n",
      "epoch: 17, iter: 2100, loss: 4.133947849273682\n",
      "epoch: 17, iter: 2200, loss: 4.278535842895508\n",
      "epoch: 17, iter: 2300, loss: 4.263110160827637\n",
      "epoch: 17, iter: 2400, loss: 4.246827602386475\n",
      "epoch: 17, iter: 2500, loss: 4.275845527648926\n",
      "epoch: 17, iter: 2600, loss: 4.228546142578125\n",
      "epoch: 17, iter: 2700, loss: 4.248086452484131\n",
      "epoch: 17, iter: 2800, loss: 4.3144683837890625\n",
      "epoch: 17, iter: 2900, loss: 4.29410457611084\n",
      "epoch: 17, iter: 3000, loss: 4.361262798309326\n",
      "epoch: 17, iter: 3100, loss: 4.297367095947266\n",
      "epoch: 17, iter: 3200, loss: 4.322575569152832\n",
      "epoch: 17, iter: 3300, loss: 4.338507652282715\n",
      "epoch: 17, iter: 3400, loss: 4.2754716873168945\n",
      "epoch: 17, iter: 3500, loss: 4.234384536743164\n",
      "epoch: 17, iter: 3600, loss: 4.165439128875732\n",
      "epoch: 17, iter: 3700, loss: 4.210566520690918\n",
      "epoch: 17, iter: 3800, loss: 4.230894088745117\n",
      "epoch: 17, iter: 3900, loss: 4.294586658477783\n",
      "epoch: 17, iter: 4000, loss: 4.275312423706055\n",
      "epoch: 17, iter: 4100, loss: 4.268275737762451\n",
      "epoch: 17, iter: 4200, loss: 4.489363670349121\n",
      "epoch: 17, iter: 4300, loss: 4.210969924926758\n",
      "epoch: 17, iter: 4400, loss: 4.210263729095459\n",
      "epoch: 17, iter: 4500, loss: 4.296440124511719\n",
      "epoch: 17, iter: 4600, loss: 4.164565086364746\n",
      "epoch: 17, iter: 4700, loss: 4.341256141662598\n",
      "epoch: 17, iter: 4800, loss: 4.1301374435424805\n",
      "epoch: 17, iter: 4900, loss: 4.400386810302734\n",
      "epoch: 17, iter: 5000, loss: 4.170429706573486\n",
      "epoch: 17, iter: 5100, loss: 4.276830673217773\n",
      "epoch: 17, iter: 5200, loss: 4.294073104858398\n",
      "epoch: 17, iter: 5300, loss: 4.097858428955078\n",
      "epoch: 17, iter: 5400, loss: 4.417356967926025\n",
      "epoch: 17, iter: 5500, loss: 4.287606716156006\n",
      "epoch: 17, iter: 5600, loss: 4.194846153259277\n",
      "epoch: 17, iter: 5700, loss: 4.515624523162842\n",
      "epoch: 17, iter: 5800, loss: 4.209192752838135\n",
      "epoch: 17, iter: 5900, loss: 4.372380256652832\n",
      "epoch: 17, iter: 6000, loss: 4.349958419799805\n",
      "epoch: 17, iter: 6100, loss: 4.303509712219238\n",
      "epoch: 17, iter: 6200, loss: 4.342122554779053\n",
      "epoch: 17, iter: 6300, loss: 4.299076080322266\n",
      "epoch: 17, iter: 6400, loss: 4.22802734375\n",
      "epoch: 17, iter: 6500, loss: 4.214602947235107\n",
      "epoch: 17, iter: 6600, loss: 4.540099143981934\n",
      "epoch: 17, iter: 6700, loss: 4.338618755340576\n",
      "epoch: 17, iter: 6800, loss: 4.224594593048096\n",
      "epoch: 17, iter: 6900, loss: 4.315586090087891\n",
      "epoch: 17, iter: 7000, loss: 4.360118865966797\n",
      "epoch: 17, iter: 7100, loss: 4.363470554351807\n",
      "epoch: 17, iter: 7200, loss: 4.275907516479492\n",
      "epoch: 17, iter: 7300, loss: 4.192506790161133\n",
      "epoch = 17\n",
      "totalloss = \n",
      "tensor(31693.9902, grad_fn=<AddBackward0>)\n",
      "epoch: 18, iter: 0, loss: 4.224139213562012\n",
      "epoch: 18, iter: 100, loss: 4.1501593589782715\n",
      "epoch: 18, iter: 200, loss: 4.283255100250244\n",
      "epoch: 18, iter: 300, loss: 4.2863593101501465\n",
      "epoch: 18, iter: 400, loss: 4.239715576171875\n",
      "epoch: 18, iter: 500, loss: 4.313211441040039\n",
      "epoch: 18, iter: 600, loss: 4.221995830535889\n",
      "epoch: 18, iter: 700, loss: 4.280971050262451\n",
      "epoch: 18, iter: 800, loss: 4.330379009246826\n",
      "epoch: 18, iter: 900, loss: 4.239875793457031\n",
      "epoch: 18, iter: 1000, loss: 4.344396591186523\n",
      "epoch: 18, iter: 1100, loss: 4.209329605102539\n",
      "epoch: 18, iter: 1200, loss: 4.150824546813965\n",
      "epoch: 18, iter: 1300, loss: 4.264146327972412\n",
      "epoch: 18, iter: 1400, loss: 4.085168361663818\n",
      "epoch: 18, iter: 1500, loss: 4.2872419357299805\n",
      "epoch: 18, iter: 1600, loss: 4.356403827667236\n",
      "epoch: 18, iter: 1700, loss: 4.383203506469727\n",
      "epoch: 18, iter: 1800, loss: 4.110520362854004\n",
      "epoch: 18, iter: 1900, loss: 4.293276786804199\n",
      "epoch: 18, iter: 2000, loss: 4.188839912414551\n",
      "epoch: 18, iter: 2100, loss: 4.15782356262207\n",
      "epoch: 18, iter: 2200, loss: 4.2264275550842285\n",
      "epoch: 18, iter: 2300, loss: 4.296703815460205\n",
      "epoch: 18, iter: 2400, loss: 4.231365203857422\n",
      "epoch: 18, iter: 2500, loss: 4.10537576675415\n",
      "epoch: 18, iter: 2600, loss: 4.33404541015625\n",
      "epoch: 18, iter: 2700, loss: 4.1821770668029785\n",
      "epoch: 18, iter: 2800, loss: 4.375590801239014\n",
      "epoch: 18, iter: 2900, loss: 4.294037342071533\n",
      "epoch: 18, iter: 3000, loss: 4.089282989501953\n",
      "epoch: 18, iter: 3100, loss: 4.212918281555176\n",
      "epoch: 18, iter: 3200, loss: 4.246982574462891\n",
      "epoch: 18, iter: 3300, loss: 4.2278852462768555\n",
      "epoch: 18, iter: 3400, loss: 4.299232482910156\n",
      "epoch: 18, iter: 3500, loss: 4.282252788543701\n",
      "epoch: 18, iter: 3600, loss: 4.267080783843994\n",
      "epoch: 18, iter: 3700, loss: 4.314005374908447\n",
      "epoch: 18, iter: 3800, loss: 4.325477600097656\n",
      "epoch: 18, iter: 3900, loss: 4.409708023071289\n",
      "epoch: 18, iter: 4000, loss: 4.3273749351501465\n",
      "epoch: 18, iter: 4100, loss: 4.493020057678223\n",
      "epoch: 18, iter: 4200, loss: 4.151939868927002\n",
      "epoch: 18, iter: 4300, loss: 4.2835164070129395\n",
      "epoch: 18, iter: 4400, loss: 4.075639247894287\n",
      "epoch: 18, iter: 4500, loss: 4.3751630783081055\n",
      "epoch: 18, iter: 4600, loss: 4.189986705780029\n",
      "epoch: 18, iter: 4700, loss: 4.429660797119141\n",
      "epoch: 18, iter: 4800, loss: 4.2339959144592285\n",
      "epoch: 18, iter: 4900, loss: 4.385375499725342\n",
      "epoch: 18, iter: 5000, loss: 4.1793365478515625\n",
      "epoch: 18, iter: 5100, loss: 4.375876426696777\n",
      "epoch: 18, iter: 5200, loss: 4.2557878494262695\n",
      "epoch: 18, iter: 5300, loss: 4.195377826690674\n",
      "epoch: 18, iter: 5400, loss: 4.323203086853027\n",
      "epoch: 18, iter: 5500, loss: 4.3070173263549805\n",
      "epoch: 18, iter: 5600, loss: 4.284579277038574\n",
      "epoch: 18, iter: 5700, loss: 4.411810874938965\n",
      "epoch: 18, iter: 5800, loss: 4.3545732498168945\n",
      "epoch: 18, iter: 5900, loss: 4.22097110748291\n",
      "epoch: 18, iter: 6000, loss: 4.289623260498047\n",
      "epoch: 18, iter: 6100, loss: 4.272171497344971\n",
      "epoch: 18, iter: 6200, loss: 4.205672740936279\n",
      "epoch: 18, iter: 6300, loss: 4.284517765045166\n",
      "epoch: 18, iter: 6400, loss: 4.376176834106445\n",
      "epoch: 18, iter: 6500, loss: 4.255791664123535\n",
      "epoch: 18, iter: 6600, loss: 4.096146583557129\n",
      "epoch: 18, iter: 6700, loss: 4.220566272735596\n",
      "epoch: 18, iter: 6800, loss: 4.139614582061768\n",
      "epoch: 18, iter: 6900, loss: 4.037737846374512\n",
      "epoch: 18, iter: 7000, loss: 4.296940803527832\n",
      "epoch: 18, iter: 7100, loss: 4.290162086486816\n",
      "epoch: 18, iter: 7200, loss: 4.3756561279296875\n",
      "epoch: 18, iter: 7300, loss: 4.314019203186035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18\n",
      "totalloss = \n",
      "tensor(31330.9316, grad_fn=<AddBackward0>)\n",
      "epoch: 19, iter: 0, loss: 4.231967926025391\n",
      "epoch: 19, iter: 100, loss: 4.125585079193115\n",
      "epoch: 19, iter: 200, loss: 4.241580963134766\n",
      "epoch: 19, iter: 300, loss: 4.218646049499512\n",
      "epoch: 19, iter: 400, loss: 4.19606351852417\n",
      "epoch: 19, iter: 500, loss: 4.195826053619385\n",
      "epoch: 19, iter: 600, loss: 4.358565330505371\n",
      "epoch: 19, iter: 700, loss: 4.118013858795166\n",
      "epoch: 19, iter: 800, loss: 4.197022438049316\n",
      "epoch: 19, iter: 900, loss: 4.226846694946289\n",
      "epoch: 19, iter: 1000, loss: 4.282125949859619\n",
      "epoch: 19, iter: 1100, loss: 4.248346328735352\n",
      "epoch: 19, iter: 1200, loss: 4.002085208892822\n",
      "epoch: 19, iter: 1300, loss: 4.126471519470215\n",
      "epoch: 19, iter: 1400, loss: 4.2166523933410645\n",
      "epoch: 19, iter: 1500, loss: 4.01594877243042\n",
      "epoch: 19, iter: 1600, loss: 4.335887432098389\n",
      "epoch: 19, iter: 1700, loss: 4.189471244812012\n",
      "epoch: 19, iter: 1800, loss: 4.254294395446777\n",
      "epoch: 19, iter: 1900, loss: 4.166607856750488\n",
      "epoch: 19, iter: 2000, loss: 4.249138832092285\n",
      "epoch: 19, iter: 2100, loss: 4.151273727416992\n",
      "epoch: 19, iter: 2200, loss: 4.4235992431640625\n",
      "epoch: 19, iter: 2300, loss: 4.4186601638793945\n",
      "epoch: 19, iter: 2400, loss: 4.1168060302734375\n",
      "epoch: 19, iter: 2500, loss: 4.130300998687744\n",
      "epoch: 19, iter: 2600, loss: 3.9764745235443115\n",
      "epoch: 19, iter: 2700, loss: 4.293688774108887\n",
      "epoch: 19, iter: 2800, loss: 3.9802134037017822\n",
      "epoch: 19, iter: 2900, loss: 4.283014297485352\n",
      "epoch: 19, iter: 3000, loss: 4.313117504119873\n",
      "epoch: 19, iter: 3100, loss: 4.148185729980469\n",
      "epoch: 19, iter: 3200, loss: 4.308361053466797\n",
      "epoch: 19, iter: 3300, loss: 4.222132205963135\n",
      "epoch: 19, iter: 3400, loss: 4.323543071746826\n",
      "epoch: 19, iter: 3500, loss: 4.202454566955566\n",
      "epoch: 19, iter: 3600, loss: 4.138665199279785\n",
      "epoch: 19, iter: 3700, loss: 4.25904655456543\n",
      "epoch: 19, iter: 3800, loss: 4.077448844909668\n",
      "epoch: 19, iter: 3900, loss: 4.123986721038818\n",
      "epoch: 19, iter: 4000, loss: 4.170047760009766\n",
      "epoch: 19, iter: 4100, loss: 4.2814202308654785\n",
      "epoch: 19, iter: 4200, loss: 4.444299221038818\n",
      "epoch: 19, iter: 4300, loss: 4.164573669433594\n",
      "epoch: 19, iter: 4400, loss: 4.3418378829956055\n",
      "epoch: 19, iter: 4500, loss: 4.3023247718811035\n",
      "epoch: 19, iter: 4600, loss: 4.355419635772705\n",
      "epoch: 19, iter: 4700, loss: 4.428145408630371\n",
      "epoch: 19, iter: 4800, loss: 4.358786106109619\n",
      "epoch: 19, iter: 4900, loss: 4.218434810638428\n",
      "epoch: 19, iter: 5000, loss: 4.173384666442871\n",
      "epoch: 19, iter: 5100, loss: 4.010417461395264\n",
      "epoch: 19, iter: 5200, loss: 4.271584510803223\n",
      "epoch: 19, iter: 5300, loss: 4.139777660369873\n",
      "epoch: 19, iter: 5400, loss: 3.9987268447875977\n",
      "epoch: 19, iter: 5500, loss: 4.22545051574707\n",
      "epoch: 19, iter: 5600, loss: 3.917553186416626\n",
      "epoch: 19, iter: 5700, loss: 4.308830738067627\n",
      "epoch: 19, iter: 5800, loss: 4.138437747955322\n",
      "epoch: 19, iter: 5900, loss: 4.107089042663574\n",
      "epoch: 19, iter: 6000, loss: 4.349374771118164\n",
      "epoch: 19, iter: 6100, loss: 4.123042583465576\n",
      "epoch: 19, iter: 6200, loss: 4.273464202880859\n",
      "epoch: 19, iter: 6300, loss: 4.188390254974365\n",
      "epoch: 19, iter: 6400, loss: 4.063138961791992\n",
      "epoch: 19, iter: 6500, loss: 4.28576135635376\n",
      "epoch: 19, iter: 6600, loss: 4.210772514343262\n",
      "epoch: 19, iter: 6700, loss: 4.202592849731445\n",
      "epoch: 19, iter: 6800, loss: 4.283422946929932\n",
      "epoch: 19, iter: 6900, loss: 4.368777751922607\n",
      "epoch: 19, iter: 7000, loss: 3.970529079437256\n",
      "epoch: 19, iter: 7100, loss: 4.347603797912598\n",
      "epoch: 19, iter: 7200, loss: 4.090150833129883\n",
      "epoch: 19, iter: 7300, loss: 4.338759422302246\n",
      "epoch = 19\n",
      "totalloss = \n",
      "tensor(30981.4902, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EPOCHS):\n",
    "    totalloss = 0\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        if  len(input_labels) == 1:\n",
    "            break\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalloss = totalloss+loss\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout: \n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "    print('epoch = %d'%e)\n",
    "    print('totalloss = ')\n",
    "    print(totalloss)\n",
    "    embedding_weights = model.input_embeddings() # 调用最终训练好的embeding词向量\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights) # 保存参数\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE)) # 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights.shape)\n",
    "#小数据范围进行操作的时候100个单词，5个数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  dists(word1,word2):\n",
    "    #print('word1 = '+str(word1))\n",
    "    #print('word2 = '+str(word2))\n",
    "    data1 = word_to_idx[word1]\n",
    "    data2 = word_to_idx[word2]\n",
    "    totaldis = 0\n",
    "    weight1 = embedding_weights[data1]\n",
    "    weight2 = embedding_weights[data2]\n",
    "    totaldis = 0.0\n",
    "    for  i  in  range(len(weight1)):\n",
    "        totaldis = totaldis + (weight1[i]-weight2[i])**2\n",
    "    #print('totaldis = '+str(totaldis))\n",
    "    #print(type(totaldis))\n",
    "    totaldis = float(totaldis)\n",
    "    return  totaldis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n",
      "situation1\n"
     ]
    }
   ],
   "source": [
    "file = open('pku_sim_test.txt')\n",
    "line = file.readline()\n",
    "resultdata = []\n",
    "maxdata = -float('inf')\n",
    "mindata = float('inf')\n",
    "word_text = []\n",
    "while  line:\n",
    "    currents = line.split()\n",
    "    flag = False\n",
    "    word_text.append(currents)\n",
    "    if  currents[0]  not  in  vocab  or  currents[1]  not  in  vocab:\n",
    "        print('situation1')\n",
    "        resultdata.append(1)\n",
    "        line = file.readline()\n",
    "        continue\n",
    "    else:\n",
    "        dij = dists(currents[0],currents[1])\n",
    "        resultdata.append(dij)\n",
    "        maxdata = max(maxdata,dij)\n",
    "        mindata = min(mindata,dij)\n",
    "    line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2.609468066068871, 1, 0.23469488037391367, 6.795081356006434, 7.62839317046698, 1, 0.1792622899531066, 0.3939660994643835, 1, 0.11211042426600694, 1, 1.0250852894912623, 1, 1, 0.00932326752781728, 1, 1, 1, 0.200750405395679, 0.4222526225172305, 4.491168386149611, 1, 1.8555508986523117, 1, 3.3875406217516635, 17.965278541979643, 2.682956947395615, 1, 2.1288477887292396, 1, 0.7238994279253748, 1, 0.5746111002219398, 1, 6.129965165086044, 1, 0.15434323129832977, 0.14621075269419148, 1, 3.3060810272489194, 12.643069605200328, 1, 1, 0.00857738861604728, 9.05218738091135, 1, 2.0546670449216538, 1, 0.2643773551007114, 1, 0.2226575756924161, 1, 0.1580793205799436, 1, 1, 18.333750869988776, 1, 0.76045406847337, 1, 1, 0.5991878867234561, 0.6696929959193889, 1, 0.0355792987510922, 0.07926148554443214, 1, 1, 6.803184177955227, 1, 0.020196280774598292, 0.046281690346223356, 0.1749503404057901, 0.028705608818952778, 3.2725556404701983, 4.706949969008707, 0.19187798973728581, 3.5051193580247544, 0.22525384007601018, 5.68997726158128, 0.00785298445128568, 0.14767022334347876, 1.0411912593788832, 1, 1, 1, 1.256491698957305, 0.45289861405832504, 6.630455157943775, 1, 1, 0.034081124781503616, 1, 0.03643281232419414, 1, 0.030182148199882068, 9.094805672082165, 0.8735508358250159, 0.6022855584792437, 1, 1.9248116213375785, 1, 0.4085427446617311, 0.8994829362501728, 1, 1, 1, 1, 1, 2.74219923118913, 0.012079767101767752, 1, 0.05613595269397027, 0.1274124853737102, 2.932666666639106, 1, 2.6109088894474306, 1, 1, 0.6474721375254014, 1, 3.9261499212753193, 0.3986889879728532, 1, 1, 0.09554073262712295, 1, 1, 0.9838702819959796, 0.04577818459456886, 1, 0.002469766486927666, 1, 14.93304414075193, 2.4267402677892687, 1, 0.0215509373524279, 0.16183382660470091, 1, 1, 0.3160845893261862, 4.4554187954933875, 0.49800691522926416, 0.25140882706263645, 1, 1, 1, 31.301017074135313, 0.19022884658506628, 0.8557240923277694, 1, 0.0797480317113944, 0.12937980398837565, 0.02930224972748065, 2.9170863091869474, 5.914501378805418, 1, 0.07638077080285392, 1, 0.008748721871980556, 1, 1, 0.6886821622390888, 1, 0.7934632645987232, 1, 0.36705587142920465, 1, 1, 1.5623938692498083, 11.399640922519433, 0.9698446786734495, 0.6826206757332047, 1, 1, 0.2179820465859208, 1, 0.9335644361716617, 0.007971253991525235, 3.163933436261815, 0.03980658783282953, 1, 1, 0.20291881671012418, 1, 0.3434363633149227, 1.383341761044755, 0.14267718928196424, 1, 0.4558898959337186, 0.01177930668933629, 1, 1, 7.9123453435979885, 1, 1, 0.5399992433540485, 2.9934446323515695, 1, 1.904113772373344, 0.29358461022235494, 1.108274734025688, 0.5682603751728854, 0.04395799427897801, 1, 0.02193400738960797, 1, 1, 1.0024082782226509, 0.08932954856865785, 2.6872220486652862, 1.8233295811556993, 8.814290484274792, 1, 0.038575426150611616, 1, 0.5254584926428516, 2.7104960239340805, 1, 1, 0.04808035555188116, 0.00449334628418836, 5.264130762178826, 2.037366014132018, 1, 1.1643263463313673, 10.393957904160779, 0.02049538441159139, 1.2656790631479924, 0.31171290996895534, 0.021582732428901845, 1, 1, 0.8010604878992071, 0.7330463601433074, 1, 0.2638430544187333, 0.6260141218349144, 0.09407403359935555, 1, 1.6378768614944992, 0.17695690168515305, 25.028199358948225, 1, 1, 0.46536636885482163, 0.05177892385946058, 0.3897783110824424, 1, 3.0008752842276465, 0.6593851107641389, 1, 17.016937716941612, 19.09694451874818, 0.08488910463779184, 8.039328335613153, 0.31662110811630134, 6.503599294360294, 1, 1, 1, 0.36990516254315803, 0.006617096761700433, 0.8101452661917035, 0.014766526088923048, 0.01380160601574014, 0.15471362552916285, 1, 7.184165873374443, 0.015244247616916097, 2.061993379540617, 0.012013795140560526, 0.07943776886220641, 1, 0.13031474493294104, 0.006579101244795982, 2.0138591098745087, 1, 1, 1, 1, 0.09339118222148697, 0.11811491336575594, 0.06876043774573101, 15.626735750343029, 1, 0.08612688813437241, 0.14095754261524152, 0.06224907785217085, 1.1345027557245575, 4.936542825144001, 0.466612292091082, 1, 3.862957769194617, 0.2975534806998309, 5.854919138774359, 0.28056088890796893, 9.01473254074779, 1, 1.8410604691351415, 8.101494943380883, 0.03203901656277838, 0.3393937300648945, 1, 1.659672334697475, 1, 0.003602275934612784, 0.20375929360810846, 0.20495119874172546, 1, 1, 1, 0.007763491598133172, 0.08856992383561206, 1, 0.17361535293830277, 1, 1, 4.860708789297812, 1, 1, 2.904624074611225, 1, 1, 1, 0.35705435200208374, 0.428200586922591, 0.1829313764417339, 0.05822255797362069, 0.07753290208081211, 3.3116825224062705, 0.013683461169318383, 1.4631390223380545, 0.4715633724244523, 9.019093983152098, 0.13670232343450184, 5.822332041612559, 0.17952826324619933, 1, 0.03790257101850227, 1, 1, 0.014255530738409898, 1, 1, 1, 1, 8.102283828937658, 0.2412116177830399, 0.13121473188464305, 0.03769115545849856, 3.4099596134068406, 0.6622542185440075, 1, 1, 5.880535754023118, 1, 1, 0.004084932658892693, 0.02284440418342843, 1, 3.302240626669536, 1, 0.1983207406564489, 1, 0.3991386747322287, 1, 1, 0.034417461186402404, 0.010380435517961249, 1, 0.47956527619946604, 4.0595275574829035, 1.0316517880915672, 6.218200519242971, 11.078071785831511, 0.00829574777935721, 5.652116647994766, 0.027118905472320095, 0.17154616229108494, 0.46703570033041786, 1, 0.119060476610162, 0.12105343429424305, 1.1254548642688307, 0.4743404553253497, 0.38944173967087403, 0.020031588660053355, 4.469571051229951, 1.5479395105066611, 1, 0.004109563688459088, 1, 0.03387216750046465, 1, 3.6301272218988414, 1, 0.05724945398961287, 0.1728283457223071, 1, 1, 0.8245268055988115, 0.0699944563889715, 0.5199823343124788, 0.6502039662472947, 1.0105587107159977, 0.20961266394639883, 3.438625628841229, 0.941206962393078, 0.40013002410089876, 1, 0.3221853527319955, 0.14472893282204435, 1, 3.373888643708357, 0.0127455910075754, 26.051648096097477, 0.038462355377237856, 0.05155646787615309, 0.20994194513336556, 1, 2.34869437317767, 2.0206255390515144, 0.883425002902578, 1.3736975284347595, 1.3708001375474965, 0.008569547370635883, 0.24783684710650367, 1, 2.8670771255246086, 2.297546417019664, 1.2254482901647832, 0.046683835781986205, 0.3140863565686734, 0.004474310938658566, 0.09750997707833979, 1, 0.07932887183110182, 1, 1, 1, 1, 1, 5.944956953543445, 1, 4.158956480615128, 0.16969625691448836, 6.277845690893632, 1, 1, 0.274010942786028, 4.460112892906585, 3.4940110432015596, 0.22197649628574828, 1, 0.3402764987078425, 0.06579764809532918, 4.80016638864181, 4.316534237313492, 3.1869238111052125, 2.627343970386533, 1, 0.024545936894886548, 0.36159339645896993, 0.048805325094831806, 3.673379947156757, 2.555862837510541, 0.46804928729551876, 0.42693613946218145, 1, 0.5938055059912131, 1, 0.7997085440229283, 1, 0.058290993852748905, 1, 0.19842878907783068, 0.1567880094746955, 0.13125694350894795, 1, 1, 1, 1.620184500137581, 0.365803022252568, 2.3704583698790627, 2.2756274453629373, 1.7476700708433426, 2.3417637716259483, 0.13039949955548796, 0.03859324656835521, 0.16204978565611627, 0.04301603731356374, 0.14247763990144613, 1, 1.4528220026129184, 1, 1, 1, 0.0019401898524078357, 1]\n"
     ]
    }
   ],
   "source": [
    "print(resultdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open(\"2020140873.txt\",\"w\",encoding='utf-8') as f:\n",
    "    for  i  in  range(len(word_text)):\n",
    "        f.write(word_text[i][0]+'\\t'+word_text[i][1])\n",
    "        if(resultdata[i] != 1):\n",
    "            resultdata[i] = (10*maxdata-mindata-9*resultdata[i])/(maxdata-mindata)\n",
    "        f.write('\\t'+str(resultdata[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.301017074135313\n",
      "0.0019401898524078357\n"
     ]
    }
   ],
   "source": [
    "print(maxdata)\n",
    "print(mindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdata = \n",
      "31.301017074135313\n",
      "mindata = \n",
      "0.0019401898524078357\n"
     ]
    }
   ],
   "source": [
    "print('maxdata = ')\n",
    "print(maxdata)\n",
    "print('mindata = ')\n",
    "print(mindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.83021080e-01 -4.42046583e-01 -8.63905787e-01 ...  1.01411331e+00\n",
      "  -5.49720228e-01 -1.61939752e+00]\n",
      " [-7.13023767e-02 -3.10909569e-01  4.45584923e-01 ...  2.34064624e-01\n",
      "  -3.40479732e-01 -6.11896873e-01]\n",
      " [-3.16732973e-01  2.34555617e-01  2.22618490e-01 ...  7.67184258e-01\n",
      "   1.20999865e-01 -1.31640446e+00]\n",
      " ...\n",
      " [ 4.79613803e-03 -2.18170695e-03 -3.99669446e-03 ... -4.54436289e-03\n",
      "   4.11063991e-03 -1.03086838e-03]\n",
      " [-2.47491105e-03 -1.86349987e-03 -4.59686387e-04 ... -4.95863846e-03\n",
      "  -2.98599899e-03 -5.36078587e-04]\n",
      " [ 2.35364679e-03  4.65715397e-03 -2.40499480e-03 ...  4.06059343e-03\n",
      "   1.04251318e-04  4.71033901e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
